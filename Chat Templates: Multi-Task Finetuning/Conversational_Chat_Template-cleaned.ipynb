{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "lh_LCGYCzfcZ",
    "outputId": "b0f9e101-2ec1-4140-9ea7-c450a501b861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m368.6/491.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DGuaiQujzfZe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import re\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uxycWgOzfWr",
    "outputId": "814ed5f7-3e0f-46e1-bfb2-d3486a10c994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcpJcs1ozfUM",
    "outputId": "a6e56d58-9c06-424d-f5a8-f83dc6726da0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: facebook/blenderbot-400M-distill\n",
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the model and tokenizer\n",
    "# We'll use a smaller model for the demo, but you can replace with larger models\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "print(f\"Loading model: {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hdUD2CUtzrY3"
   },
   "outputs": [],
   "source": [
    "# Step 2: Define helper functions for chat\n",
    "\n",
    "def format_message(role, content):\n",
    "    \"\"\"Format message according to role (user or assistant)\"\"\"\n",
    "    if role == \"user\":\n",
    "        return f\"User: {content}\"\n",
    "    else:\n",
    "        return f\"Assistant: {content}\"\n",
    "\n",
    "def format_conversation(conversation):\n",
    "    \"\"\"Format the entire conversation history\"\"\"\n",
    "    return \"\\n\".join([format_message(msg[\"role\"], msg[\"content\"]) for msg in conversation])\n",
    "\n",
    "def generate_response(conversation, max_length=128):\n",
    "    \"\"\"Generate a response from the model based on conversation history\"\"\"\n",
    "    # Format the conversation history\n",
    "    formatted_conversation = format_conversation(conversation)\n",
    "\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(formatted_conversation + \"\\nAssistant:\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate a response\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode the response\n",
    "    full_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract just the assistant's response\n",
    "    assistant_response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "\n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Pk1MMWVTzyrk"
   },
   "outputs": [],
   "source": [
    "# Step 3: Create a simple chat interface\n",
    "conversation_history = []\n",
    "\n",
    "def chat_with_bot():\n",
    "    \"\"\"Interactive chat function\"\"\"\n",
    "    print(\"\\n===== Conversational Chatbot Demo =====\")\n",
    "    print(\"Type 'exit' to end the conversation\\n\")\n",
    "\n",
    "    while True:\n",
    "        # Get user input\n",
    "        user_input = input(\"User: \")\n",
    "\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"\\nThank you for chatting! Goodbye.\")\n",
    "            break\n",
    "\n",
    "        # Add user message to conversation history\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "        # Generate response\n",
    "        print(\"Assistant is thinking...\")\n",
    "        assistant_response = generate_response(conversation_history)\n",
    "\n",
    "        # Add assistant message to conversation history\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "        # Display response\n",
    "        print(f\"Assistant: {assistant_response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HdfwzGJtz1KK"
   },
   "outputs": [],
   "source": [
    "# Step 4: Implement enhanced features\n",
    "\n",
    "# 4.1 Persona customization\n",
    "def set_bot_persona(persona_description):\n",
    "    \"\"\"Set the bot's persona\"\"\"\n",
    "    # Add a system message at the beginning of the conversation\n",
    "    if len(conversation_history) == 0 or conversation_history[0][\"role\"] != \"system\":\n",
    "        conversation_history.insert(0, {\"role\": \"system\", \"content\": persona_description})\n",
    "    else:\n",
    "        conversation_history[0] = {\"role\": \"system\", \"content\": persona_description}\n",
    "    print(f\"Bot persona set to: {persona_description}\")\n",
    "\n",
    "# 4.2 Memory management\n",
    "def clear_conversation():\n",
    "    \"\"\"Clear the conversation history\"\"\"\n",
    "    conversation_history.clear()\n",
    "    print(\"Conversation history cleared.\")\n",
    "\n",
    "def summarize_conversation():\n",
    "    \"\"\"Summarize the current conversation\"\"\"\n",
    "    if len(conversation_history) <= 2:\n",
    "        return \"The conversation just started.\"\n",
    "\n",
    "    # Format the conversation for summarization\n",
    "    formatted_text = \"Summarize this conversation:\\n\\n\" + format_conversation(conversation_history)\n",
    "\n",
    "    # Use the model to generate a summary\n",
    "    inputs = tokenizer(formatted_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + 100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    summary = tokenizer.decode(output[0], skip_special_tokens=True).replace(formatted_text, \"\").strip()\n",
    "    return summary\n",
    "\n",
    "# 4.3 Context-awareness enhancement\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"Simple sentiment analysis\"\"\"\n",
    "    positive_words = [\"good\", \"great\", \"happy\", \"positive\", \"excellent\", \"wonderful\", \"love\", \"like\", \"enjoy\"]\n",
    "    negative_words = [\"bad\", \"terrible\", \"sad\", \"negative\", \"awful\", \"horrible\", \"hate\", \"dislike\", \"disappointing\"]\n",
    "\n",
    "    text = text.lower()\n",
    "    positive_count = sum(1 for word in positive_words if word in text)\n",
    "    negative_count = sum(1 for word in negative_words if word in text)\n",
    "\n",
    "    if positive_count > negative_count:\n",
    "        return \"positive\"\n",
    "    elif negative_count > positive_count:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "E4Uady7Mz45l"
   },
   "outputs": [],
   "source": [
    "# Step 5: Create an interactive demo with Colab widgets\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def interactive_chat_demo():\n",
    "    # Create widgets\n",
    "    output = widgets.Output()\n",
    "    text_input = widgets.Text(placeholder=\"Type your message here...\")\n",
    "    send_button = widgets.Button(description=\"Send\")\n",
    "    clear_button = widgets.Button(description=\"Clear Chat\")\n",
    "\n",
    "    persona_dropdown = widgets.Dropdown(\n",
    "        options=[\n",
    "            'Helpful Assistant',\n",
    "            'Travel Guide',\n",
    "            'Tech Support',\n",
    "            'Friendly Friend',\n",
    "            'Professional Colleague'\n",
    "        ],\n",
    "        value='Helpful Assistant',\n",
    "        description='Bot Persona:'\n",
    "    )\n",
    "\n",
    "    # Display widgets\n",
    "    display(HTML(\"<h3>Conversational Chatbot</h3>\"))\n",
    "    display(persona_dropdown)\n",
    "    display(widgets.HBox([text_input, send_button, clear_button]))\n",
    "    display(output)\n",
    "\n",
    "    # Initialize conversation history\n",
    "    conversation = []\n",
    "    set_bot_persona(\"You are a helpful, respectful and honest assistant.\")\n",
    "\n",
    "    # Define button click handlers\n",
    "    def on_send_button_clicked(b):\n",
    "        user_message = text_input.value\n",
    "        if not user_message.strip():\n",
    "            return\n",
    "\n",
    "        text_input.value = \"\"\n",
    "\n",
    "        # Add user message to conversation\n",
    "        conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "        with output:\n",
    "            print(f\"User: {user_message}\")\n",
    "            print(\"Assistant is thinking...\")\n",
    "\n",
    "            # Generate response\n",
    "            assistant_response = generate_response(conversation)\n",
    "\n",
    "            # Add assistant message to conversation\n",
    "            conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "            print(f\"Assistant: {assistant_response}\\n\")\n",
    "\n",
    "    def on_clear_button_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            conversation.clear()\n",
    "            # Set the persona again after clearing\n",
    "            current_persona = persona_dropdown.value\n",
    "            if current_persona == \"Helpful Assistant\":\n",
    "                set_bot_persona(\"You are a helpful, respectful and honest assistant.\")\n",
    "            elif current_persona == \"Travel Guide\":\n",
    "                set_bot_persona(\"You are a knowledgeable travel guide who provides detailed information about destinations, travel tips, and local customs.\")\n",
    "            elif current_persona == \"Tech Support\":\n",
    "                set_bot_persona(\"You are a patient technical support specialist who helps users troubleshoot their computer and software issues.\")\n",
    "            elif current_persona == \"Friendly Friend\":\n",
    "                set_bot_persona(\"You are a friendly and supportive friend who offers empathy, advice, and casual conversation.\")\n",
    "            elif current_persona == \"Professional Colleague\":\n",
    "                set_bot_persona(\"You are a professional colleague who communicates in a business-appropriate manner, focusing on tasks and efficiency.\")\n",
    "            print(\"Chat cleared. You can start a new conversation.\")\n",
    "\n",
    "    def on_persona_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            with output:\n",
    "                if change['new'] == \"Helpful Assistant\":\n",
    "                    set_bot_persona(\"You are a helpful, respectful and honest assistant.\")\n",
    "                elif change['new'] == \"Travel Guide\":\n",
    "                    set_bot_persona(\"You are a knowledgeable travel guide who provides detailed information about destinations, travel tips, and local customs.\")\n",
    "                elif change['new'] == \"Tech Support\":\n",
    "                    set_bot_persona(\"You are a patient technical support specialist who helps users troubleshoot their computer and software issues.\")\n",
    "                elif change['new'] == \"Friendly Friend\":\n",
    "                    set_bot_persona(\"You are a friendly and supportive friend who offers empathy, advice, and casual conversation.\")\n",
    "                elif change['new'] == \"Professional Colleague\":\n",
    "                    set_bot_persona(\"You are a professional colleague who communicates in a business-appropriate manner, focusing on tasks and efficiency.\")\n",
    "                print(f\"Bot persona changed to: {change['new']}\")\n",
    "\n",
    "    # Connect the handlers\n",
    "    send_button.on_click(on_send_button_clicked)\n",
    "    clear_button.on_click(on_clear_button_clicked)\n",
    "    persona_dropdown.observe(on_persona_change)\n",
    "\n",
    "    # Allow pressing Enter to send message\n",
    "    def on_text_change(change):\n",
    "        if change['type'] == 'change' and change['name'] == 'value':\n",
    "            if change['new'].endswith('\\n'):\n",
    "                text_input.value = change['new'].rstrip('\\n')\n",
    "                on_send_button_clicked(None)\n",
    "\n",
    "    text_input.observe(on_text_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wrRBcJ7uz8Hi"
   },
   "outputs": [],
   "source": [
    "# Step 6: Evaluation and testing functions\n",
    "\n",
    "def evaluate_response_quality(response, criteria=None):\n",
    "    \"\"\"Evaluate the quality of the bot's response based on criteria\"\"\"\n",
    "    if criteria is None:\n",
    "        criteria = {\n",
    "            \"relevance\": \"Is the response relevant to the user's message?\",\n",
    "            \"helpfulness\": \"Is the response helpful?\",\n",
    "            \"fluency\": \"Is the response well-written and fluent?\",\n",
    "            \"safety\": \"Is the response safe and appropriate?\"\n",
    "        }\n",
    "\n",
    "    results = {}\n",
    "    print(\"Response Quality Evaluation:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for key, description in criteria.items():\n",
    "        # In a real application, this would use more sophisticated evaluation\n",
    "        # Here we're just using a simple heuristic based on length and content\n",
    "        if key == \"fluency\":\n",
    "            score = min(10, max(1, len(response.split()) / 5))\n",
    "        elif key == \"relevance\":\n",
    "            score = 7  # Default score, would need context for better evaluation\n",
    "        elif key == \"helpfulness\":\n",
    "            score = min(10, max(1, len(response) / 20))\n",
    "        elif key == \"safety\":\n",
    "            # Simple check for obviously problematic content\n",
    "            unsafe_terms = [\"kill\", \"harm\", \"illegal\", \"violent\", \"dangerous\"]\n",
    "            if any(term in response.lower() for term in unsafe_terms):\n",
    "                score = 3\n",
    "            else:\n",
    "                score = 9\n",
    "\n",
    "        results[key] = score\n",
    "        print(f\"{key.capitalize()} ({description}): {score}/10\")\n",
    "\n",
    "    avg_score = sum(results.values()) / len(results)\n",
    "    print(f\"Overall Score: {avg_score:.2f}/10\")\n",
    "    return results\n",
    "\n",
    "def test_with_sample_conversations():\n",
    "    \"\"\"Test the bot with a set of sample conversation scenarios\"\"\"\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"name\": \"Greeting scenario\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"Hi there!\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Question answering\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"What are some good books to read?\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Multi-turn conversation\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"I'm planning a trip.\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"That sounds exciting! Where are you planning to go?\"},\n",
    "                {\"role\": \"user\", \"content\": \"I'm thinking about visiting Japan.\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Technical support\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"My computer is running really slow lately.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"Running Test Scenarios:\")\n",
    "    for scenario in test_scenarios:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"Scenario: {scenario['name']}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Reset conversation for each test\n",
    "        test_conversation = []\n",
    "\n",
    "        # Add the test messages\n",
    "        for msg in scenario[\"messages\"]:\n",
    "            test_conversation.append(msg)\n",
    "            print(f\"{msg['role'].capitalize()}: {msg['content']}\")\n",
    "\n",
    "        # If the last message is from the user, generate a response\n",
    "        if test_conversation[-1][\"role\"] == \"user\":\n",
    "            print(\"\\nGenerating response...\")\n",
    "            response = generate_response(test_conversation)\n",
    "            print(f\"Assistant: {response}\")\n",
    "\n",
    "            # Evaluate the response\n",
    "            evaluate_response_quality(response)\n",
    "\n",
    "    print(\"\\nTest scenarios completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450,
     "referenced_widgets": [
      "1946a16680c34113bad9a476cb860acd",
      "4487c2de355548fd81ce9aee9793b7f6",
      "aaec59814c134f77815f7149185e4049",
      "76c184dfb758452b84f7072bceedecc5",
      "27cdd69982864683a500a961ba399c5e",
      "62c7aec15e704013af1f260aad76fbdd",
      "74774d2faf814f4a99ee20319f66a840",
      "be831b6db246491e93ccbc39597ecb62",
      "55bbe45ff9c44546b265a33e231e6374",
      "ecd74353692c49fea4f5cad25d148bd8",
      "1dca39dcfd5b429a9f67b607badaf828",
      "6f8eada4c9f24a2b8bb3a441da1e1ee2",
      "b734854e99dd4093b40880a181f6a1f3",
      "560723acd96349c1b9dc713b85ddb0f9",
      "c1e54ccab97748fab35a235258c53784",
      "beb830cc7784455ca159e5036ba226ee"
     ]
    },
    "id": "c15HiwHpz_J1",
    "outputId": "13d65b4a-6f24-4d8d-9dd9-d74ecd3554c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot System Ready!\n",
      "Choose an option to continue:\n",
      "1: Start interactive text chat\n",
      "2: Run interactive demo with widgets (works best in Colab)\n",
      "3: Run test scenarios\n",
      "4: Exit\n",
      "Enter your choice (1-4): 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Conversational Chatbot</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1946a16680c34113bad9a476cb860acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Bot Persona:', options=('Helpful Assistant', 'Travel Guide', 'Tech Support', 'Friendly F…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c184dfb758452b84f7072bceedecc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', placeholder='Type your message here...'), Button(description='Send', style=Butto…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e54ccab97748fab35a235258c53784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot persona set to: You are a helpful, respectful and honest assistant.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Additional utility functions\n",
    "\n",
    "def save_conversation(filename=\"chatbot_conversation.txt\"):\n",
    "    \"\"\"Save the current conversation to a file\"\"\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(format_conversation(conversation_history))\n",
    "    print(f\"Conversation saved to {filename}\")\n",
    "\n",
    "def load_conversation(filename=\"chatbot_conversation.txt\"):\n",
    "    \"\"\"Load a conversation from a file\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        # Parse the content back into conversation format\n",
    "        messages = []\n",
    "        for line in content.split(\"\\n\"):\n",
    "            if line.startswith(\"User: \"):\n",
    "                messages.append({\"role\": \"user\", \"content\": line[6:]})\n",
    "            elif line.startswith(\"Assistant: \"):\n",
    "                messages.append({\"role\": \"assistant\", \"content\": line[11:]})\n",
    "\n",
    "        # Update the conversation history\n",
    "        conversation_history.clear()\n",
    "        conversation_history.extend(messages)\n",
    "        print(f\"Conversation loaded from {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading conversation: {e}\")\n",
    "\n",
    "def display_metrics():\n",
    "    \"\"\"Display metrics about the conversation\"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"No conversation history to analyze.\")\n",
    "        return\n",
    "\n",
    "    # Count messages by role\n",
    "    user_msgs = sum(1 for msg in conversation_history if msg[\"role\"] == \"user\")\n",
    "    assistant_msgs = sum(1 for msg in conversation_history if msg[\"role\"] == \"assistant\")\n",
    "\n",
    "    # Calculate average message length\n",
    "    user_lengths = [len(msg[\"content\"]) for msg in conversation_history if msg[\"role\"] == \"user\"]\n",
    "    assistant_lengths = [len(msg[\"content\"]) for msg in conversation_history if msg[\"role\"] == \"assistant\"]\n",
    "\n",
    "    user_avg_len = sum(user_lengths) / len(user_lengths) if user_lengths else 0\n",
    "    assistant_avg_len = sum(assistant_lengths) / len(assistant_lengths) if assistant_lengths else 0\n",
    "\n",
    "    # Response time analysis would go here in a real application\n",
    "\n",
    "    # Display the metrics\n",
    "    print(\"\\n===== Conversation Metrics =====\")\n",
    "    print(f\"Total messages: {len(conversation_history)}\")\n",
    "    print(f\"User messages: {user_msgs}\")\n",
    "    print(f\"Assistant messages: {assistant_msgs}\")\n",
    "    print(f\"Average user message length: {user_avg_len:.1f} characters\")\n",
    "    print(f\"Average assistant message length: {assistant_avg_len:.1f} characters\")\n",
    "\n",
    "    # Simple sentiment analysis of the conversation\n",
    "    all_user_text = \" \".join([msg[\"content\"] for msg in conversation_history if msg[\"role\"] == \"user\"])\n",
    "    sentiment = analyze_sentiment(all_user_text)\n",
    "    print(f\"Overall conversation sentiment: {sentiment}\")\n",
    "\n",
    "# Main execution block - choose what to run\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nChatbot System Ready!\")\n",
    "    print(\"Choose an option to continue:\")\n",
    "    print(\"1: Start interactive text chat\")\n",
    "    print(\"2: Run interactive demo with widgets (works best in Colab)\")\n",
    "    print(\"3: Run test scenarios\")\n",
    "    print(\"4: Exit\")\n",
    "\n",
    "    choice = input(\"Enter your choice (1-4): \")\n",
    "\n",
    "    if choice == \"1\":\n",
    "        chat_with_bot()\n",
    "    elif choice == \"2\":\n",
    "        interactive_chat_demo()\n",
    "    elif choice == \"3\":\n",
    "        test_with_sample_conversations()\n",
    "    else:\n",
    "        print(\"Exiting program. Goodbye!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
