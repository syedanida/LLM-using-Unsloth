{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "BA2DUgE8JSSI",
    "outputId": "79420ee7-2b03-4047-8276-6a8ed16fdd00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets rouge evaluate transformers wandb nltk rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ha_FL9oxHpN9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "import nltk\n",
    "import rouge\n",
    "import wandb\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "klYK69FNH9Y-",
    "outputId": "8c931a03-fd42-4796-80a8-e20055c3a1ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "id": "ZK__3kY2IUq6",
    "outputId": "c924ee13-f35a-4957-b470-08a272b48cb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msyedanida-khader\u001b[0m (\u001b[33msyedanida-khader-san-jose-state-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250416_180126-ika8mu01</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/syedanida-khader-san-jose-state-university/multiple-dataset-fine-tuning/runs/ika8mu01' target=\"_blank\">revived-resonance-12</a></strong> to <a href='https://wandb.ai/syedanida-khader-san-jose-state-university/multiple-dataset-fine-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/syedanida-khader-san-jose-state-university/multiple-dataset-fine-tuning' target=\"_blank\">https://wandb.ai/syedanida-khader-san-jose-state-university/multiple-dataset-fine-tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/syedanida-khader-san-jose-state-university/multiple-dataset-fine-tuning/runs/ika8mu01' target=\"_blank\">https://wandb.ai/syedanida-khader-san-jose-state-university/multiple-dataset-fine-tuning/runs/ika8mu01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/syedanida-khader-san-jose-state-university/multiple-dataset-fine-tuning/runs/ika8mu01?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x794148cc4cd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up Weights & Biases for tracking experiments (optional but recommended)\n",
    "wandb.init(project=\"multiple-dataset-fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Il09NNuoIVoY",
    "outputId": "7e66818c-a4b1-4d20-8190-26e60be4ff68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zbEcyBd1IXMz"
   },
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoDYNZiPIXKa",
    "outputId": "e1f73ab6-5e91-48a3-d271-881e0344b64d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "# Using T5 as it can handle multiple tasks in a seq2seq format\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TkRaQMytIXH_"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# 1. CNN/DailyMail for summarization\n",
    "cnn_dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9tzz_T3sIZgx",
    "outputId": "5aaedf15-e8f5-4ea3-cdf6-b9a6b97c3c3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully!\n",
      "CNN/DailyMail - Train: 287113, Validation: 13368, Test: 11490\n",
      "SST-2 - Train: 67349, Validation: 872, Test: 1821\n"
     ]
    }
   ],
   "source": [
    "# 2. GLUE SST-2 for sentiment classification\n",
    "sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "print(\"Datasets loaded successfully!\")\n",
    "print(f\"CNN/DailyMail - Train: {len(cnn_dataset['train'])}, Validation: {len(cnn_dataset['validation'])}, Test: {len(cnn_dataset['test'])}\")\n",
    "print(f\"SST-2 - Train: {len(sst2_dataset['train'])}, Validation: {len(sst2_dataset['validation'])}, Test: {len(sst2_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298,
     "referenced_widgets": [
      "fd8611e739a64ee8a7b89b5b8c73158a",
      "9ff9bfdd7b4f4784bef5101b07aa1caf",
      "e7f58582e97a42f6863a27473adb60af",
      "3c014b43af334490b42804c47e1228a0",
      "e42ee5dad30c46479dec24293c5a3753",
      "419138b8e5d945ed81fa3fa9504ae7c9",
      "a89b3346c3b84026be198960b3a51c1c",
      "6a768b1650404b6f90e516e48ea39550",
      "8a6c3187bb524b5f8645a934694c047e",
      "552c55695b984591ad07b411a026fe21",
      "db049bf8e707429eaa567bc5949c1f9f",
      "1238ffc408ba444c9d28cbc3ea9253a3",
      "31308aed99624db5a68ad85657b330ba",
      "cf92b95385b64ab59406ad60de7409d2",
      "808330bf66a34652b9d2a14ebc8d815a",
      "695a79cff798452b92ad8acf0f338112",
      "f3670cebee6249c3ae0051b3fe04e5f6",
      "73e119b723e14f76926f29fef0a35db8",
      "9b74079a13874f1e9239de412b6f8366",
      "2207b5945e25449199a7af8c55234557",
      "13a9896a8805419da795a3e776934bb2",
      "437664eb8cc5477f8c9ac42d15e2b266",
      "32e1311070574c65a22df1e9fa851621",
      "8342f50507364cf195c9efce5fdeb669",
      "9ee440a36922432abaaf12ce554a423a",
      "2b5da6cb07874edeb5addd5da6ad07dd",
      "b088e2b46b1240dba02e4801ef7007ff",
      "20e5f5ea7dfc47a5b7ced999c0b2f961",
      "1aba3673f1a941ed9d8033724621fd32",
      "2e774f680d89424c83d99e3c57ffd607",
      "40403e021a9b47a2a321de7301f9f0e6",
      "187c099c1abe49069610a57555e666f5",
      "457d0af8d9424768afeb13bd662acc9d",
      "25eb8a749f96400399a664114fbfa621",
      "18650d0b81fd44a6b410931c115c251f",
      "1c1bfe380271413bbb0a947f9c46451d",
      "d07c1c5d57e4409d9bf05da8f8369f46",
      "4474584c9733454d8c433bee864ef512",
      "052180e401824b6fbca45af1fdf83f34",
      "04fe3e82d9a34d3f9d1ef0296e1787fd",
      "9248a802abb841a785101055930a02d2",
      "457c5cadb70b4c24861c513b47399cc8",
      "b5a798ee9e3d4dcf94b1d4c80dfd1377",
      "f859d91561ff42709a55da7d685eb43c",
      "0a780274f36940609eccc20d4dbefbdc",
      "4b7191a71ddd478c80714286536d19b8",
      "d17f8abd83624efbbe1092b8a357618c",
      "00aba88f544b4af7a27b3f91181e817c",
      "7fa67e8fd57e474da7cda8fd4f57b918",
      "6575b59839874d818dd316f65079eaf4",
      "0688ce7e90c547b9b7a759e21fe72d59",
      "4ab874113d7c41efa432052f33753c59",
      "c1b3d7db7dac4127a57cb43573bb5aa9",
      "a36848969ddf4a51b8f22fb97331d907",
      "ff561a45cd4842d5986859b51ad77c77",
      "7d48a679cd624a57b93dae3460c04913",
      "69be8621bd3e41fca0b6884cfc2e409a",
      "104da54f3afb4710a018d254ad282b07",
      "9c3a65d52ce74571844b825855b6dddb",
      "70c0e33804174316a9461e74a08064cf",
      "263354681c8545dbbb4b4fb59bd7629c",
      "15e29ca9e3c3403e96c871366365ee25",
      "d6156c9c8e124501bf70360a0fd69e98",
      "3051cdf0c3874749a0225c3bec1b6733",
      "0f8e649b98ae4d54b56f81a4eb0c9552",
      "56d061109a8b4335be96a80d5c77e997"
     ]
    },
    "id": "XGPQBZ7NxWp4",
    "outputId": "53393c12-f24c-4daa-ae17-63901457767d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8611e739a64ee8a7b89b5b8c73158a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1238ffc408ba444c9d28cbc3ea9253a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e1311070574c65a22df1e9fa851621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25eb8a749f96400399a664114fbfa621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a780274f36940609eccc20d4dbefbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d48a679cd624a57b93dae3460c04913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined training set size: 4000\n",
      "Combined validation set size: 1000\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the datasets\n",
    "cnn_processed = cnn_dataset.map(preprocess_cnn_dailymail, batched=True)  # Preprocess CNN/DailyMail\n",
    "sst2_processed = sst2_dataset.map(preprocess_sst2, batched=True)      # Preprocess SST-2\n",
    "\n",
    "cnn_sample_size = min(len(cnn_processed[\"train\"]), 2000)\n",
    "sst2_sample_size = min(len(sst2_processed[\"train\"]), 2000)\n",
    "\n",
    "cnn_train_subset = cnn_processed[\"train\"].shuffle(seed=42).select(range(cnn_sample_size))\n",
    "sst2_train_subset = sst2_processed[\"train\"].shuffle(seed=42).select(range(sst2_sample_size))\n",
    "\n",
    "# Combine datasets for training\n",
    "combined_train = concatenate_datasets([cnn_train_subset, sst2_train_subset])\n",
    "combined_val = concatenate_datasets([\n",
    "    cnn_processed[\"validation\"].shuffle(seed=42).select(range(min(len(cnn_processed[\"validation\"]), 500))),\n",
    "    sst2_processed[\"validation\"].shuffle(seed=42).select(range(min(len(sst2_processed[\"validation\"]), 500)))\n",
    "])\n",
    "\n",
    "print(f\"Combined training set size: {len(combined_train)}\")\n",
    "print(f\"Combined validation set size: {len(combined_val)}\")\n",
    "\n",
    "# Create a custom data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dwT99cHEIZeR"
   },
   "outputs": [],
   "source": [
    "# Function to preprocess CNN/DailyMail for summarization\n",
    "def preprocess_cnn_dailymail(examples):\n",
    "    # Add task prefix to distinguish this as a summarization task\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True)\n",
    "\n",
    "    # Tokenize targets (summaries)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"highlights\"], max_length=64, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # Add task type identifier\n",
    "    model_inputs[\"task_type\"] = [\"summarization\"] * len(inputs)\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZCIeCoq8Ij0D"
   },
   "outputs": [],
   "source": [
    "# Function to preprocess SST-2 for sentiment classification - optimized version\n",
    "def preprocess_sst2(examples):\n",
    "    batch_size = len(examples[\"sentence\"])\n",
    "\n",
    "    # Add task prefix in a more efficient way\n",
    "    inputs = [f\"classify sentiment: {sentence}\" for sentence in examples[\"sentence\"]]\n",
    "\n",
    "    # Tokenize inputs - use padding=False to avoid unnecessary padding during preprocessing\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=False  # Change from \"max_length\" to False\n",
    "    )\n",
    "\n",
    "    # Simplify label conversion\n",
    "    text_labels = [\"negative\" if label == 0 else \"positive\" for label in examples[\"label\"]]\n",
    "\n",
    "    # Tokenize targets with padding=False\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            text_labels,\n",
    "            max_length=8,\n",
    "            truncation=True,\n",
    "            padding=False  # Change from \"max_length\" to False\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # Add task type identifier efficiently\n",
    "    model_inputs[\"task_type\"] = [\"classification\"] * batch_size\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aAzzYt6jIjvG"
   },
   "outputs": [],
   "source": [
    "# Set up metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qs31BxfgIZbV"
   },
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1AqbxkqYIl3f"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Decode predictions and labels with better error handling\n",
    "    decoded_preds = []\n",
    "    try:\n",
    "        # Convert to int32 and clip values to valid token range\n",
    "        max_id = tokenizer.vocab_size - 1\n",
    "        clipped_preds = np.clip(predictions, 0, max_id).astype(np.int32)\n",
    "        decoded_preds = tokenizer.batch_decode(clipped_preds, skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        # If batch decoding fails, fall back to individual decoding with safeguards\n",
    "        for pred in predictions:\n",
    "            try:\n",
    "                # Clip values to valid token range\n",
    "                clipped_pred = np.clip(pred, 0, tokenizer.vocab_size - 1).astype(np.int32)\n",
    "                decoded_pred = tokenizer.decode(clipped_pred, skip_special_tokens=True)\n",
    "                decoded_preds.append(decoded_pred)\n",
    "            except Exception as inner_e:\n",
    "                # If a prediction can't be decoded, use an empty string\n",
    "                print(f\"Warning: Failed to decode prediction: {inner_e}\")\n",
    "                decoded_preds.append(\"\")\n",
    "\n",
    "    # Process labels (which are usually more stable)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Clean up predictions and labels\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Rest of the function remains the same...\n",
    "    classification_preds = []\n",
    "    classification_labels = []\n",
    "    summarization_preds = []\n",
    "    summarization_labels = []\n",
    "\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        if \"positive\" in label[0] or \"negative\" in label[0]:\n",
    "            # This is a classification task\n",
    "            classification_preds.append(pred)\n",
    "            classification_labels.append(label[0])\n",
    "        else:\n",
    "            # This is a summarization task\n",
    "            summarization_preds.append(pred)\n",
    "            summarization_labels.append(label[0])\n",
    "\n",
    "    # Results dictionary\n",
    "    results = {}\n",
    "\n",
    "    # Compute ROUGE for summarization if we have any summarization examples\n",
    "    if summarization_preds:\n",
    "        rouge_output = rouge.compute(\n",
    "            predictions=summarization_preds,\n",
    "            references=[[label] for label in summarization_labels],\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        results.update({k: v for k, v in rouge_output.items()})\n",
    "\n",
    "    # Compute classification metrics if we have any classification examples\n",
    "    if classification_preds:\n",
    "        # Convert text predictions to binary labels for accuracy\n",
    "        binary_preds = [\"positive\" in pred for pred in classification_preds]\n",
    "        binary_labels = [\"positive\" in label for label in classification_labels]\n",
    "\n",
    "        results[\"classification_accuracy\"] = accuracy_score(binary_labels, binary_preds)\n",
    "        results[\"classification_f1\"] = f1_score(binary_labels, binary_preds, average='binary')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "A057WoODIl1B"
   },
   "outputs": [],
   "source": [
    "# Define training arguments with corrected steps\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    fp16=True,\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\" if len(cnn_processed[\"validation\"]) > 0 else \"classification_accuracy\",\n",
    "    push_to_hub=False,\n",
    "    dataloader_num_workers=4,\n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvJ--2eBI2Na",
    "outputId": "e797d5d9-e5c2-4d57-eb53-f12545b7cbc6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-d50843d37ce8>:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_train,\n",
    "    eval_dataset=combined_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G4JdrMidI2IF",
    "outputId": "d42ab9d4-fa58-4f66-dbe5-aeb4a7046703"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./fine_tuned_multi_task_model\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model_path = \"./fine_tuned_multi_task_model\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nU1181MfI5_o",
    "outputId": "cbedad33-c17f-4575-f886-909a4c1e1ee0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Summarization Example: \n",
      "\n",
      "    The COVID-19 pandemic has dramatically changed the way we live and work.\n",
      "    Many companies hav...\n",
      "Generated Summary: \n",
      "the COVID-19 pandemic has dramatically changed the way we live and work. many companies have shifted to remote work - and schools have adopted online learning models.\n",
      "\n",
      "Classification Example: \n",
      "The movie was absolutely fantastic with great performances and an engaging storyline.\n",
      "Predicted Sentiment: Der Film war absolut fantastig mit tollen\n"
     ]
    }
   ],
   "source": [
    "# Test the model on both tasks\n",
    "def test_model_on_both_tasks(model, tokenizer):\n",
    "    model.eval()\n",
    "\n",
    "    # Test summarization\n",
    "    article = \"\"\"\n",
    "    The COVID-19 pandemic has dramatically changed the way we live and work.\n",
    "    Many companies have shifted to remote work, and schools have adopted\n",
    "    online learning models. Public health measures including social distancing\n",
    "    and mask-wearing have become commonplace in many regions. Vaccines were\n",
    "    developed in record time, but distribution challenges and vaccine hesitancy\n",
    "    remain obstacles to achieving herd immunity.\n",
    "    \"\"\"\n",
    "\n",
    "    summarization_input = tokenizer(\"summarize: \" + article, return_tensors=\"pt\").to(device)\n",
    "    summary_ids = model.generate(\n",
    "        summarization_input[\"input_ids\"],\n",
    "        max_length=75,\n",
    "        min_length=30,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Test classification\n",
    "    review = \"The movie was absolutely fantastic with great performances and an engaging storyline.\"\n",
    "    classification_input = tokenizer(\"classify sentiment: \" + review, return_tensors=\"pt\").to(device)\n",
    "    sentiment_ids = model.generate(\n",
    "        classification_input[\"input_ids\"],\n",
    "        max_length=10,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    sentiment = tokenizer.decode(sentiment_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return {\n",
    "        \"summarization_example\": article,\n",
    "        \"generated_summary\": summary,\n",
    "        \"classification_example\": review,\n",
    "        \"predicted_sentiment\": sentiment\n",
    "    }\n",
    "\n",
    "# Test the model\n",
    "test_results = test_model_on_both_tasks(model, tokenizer)\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"Summarization Example: \\n{test_results['summarization_example'][:100]}...\")\n",
    "print(f\"Generated Summary: \\n{test_results['generated_summary']}\")\n",
    "print(f\"\\nClassification Example: \\n{test_results['classification_example']}\")\n",
    "print(f\"Predicted Sentiment: {test_results['predicted_sentiment']}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
