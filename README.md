# ğŸš€ LLM Fine-Tuning & Pretraining Assignment with Unsloth

This repository contains Colab notebooks and demo videos showcasing various advanced fine-tuning, pretraining, and deployment use cases using open-weight LLMs and the [Unsloth AI](https://unsloth.ai/) ecosystem.

---

## ğŸ“ Contents

a. Fine-Tuning Various Open-Weight LLMs

b. Continued Pretraining

c. Chat Templates: Multi-Task Finetuning

d. Reward Modeling: ORPO & DPO

e. Continued Finetuning from Checkpoint

f. Mental Health Chatbot Fine-Tuning

g. Export to Ollama and Inference

h. Reasoning Model Fine-Tuning (GRPO)

---

## a) Fine-Tuning Various Open-Weight LLMs

Fine-tuned models on different tasks such as coding, chat, instruction-following, and classification.

| Model Used           | Use Case Type  | Colab Link |
|----------------------|----------------|------------|
| Llama 3.1 (8B)       | Coding Assistant | [Colab ğŸ”—]() |
| Mistral NeMo (12B)   | Instruction Chat | [Colab ğŸ”—]() |
| Phi-3.5 (mini)       | Conversational Chatbot | [Colab ğŸ”—]() |
| Gemma 2 (9B)         | Text Classification | [Colab ğŸ”—]() |

- **ğŸ¥ Video**: [Watch ğŸ¥]()

---

## b) Continued Pretraining

Using Unsloth AI, continued pretraining to teach an LLM a new language.

- **Colab Notebook**: [Link ğŸ”—]()
- **ğŸ¥ Video**: [Watch ğŸ¥]()

---

## c) Chat Templates: Multi-Task Finetuning

Explored Unsloth's chat templates to perform multiple tasks in a single finetuning run.

| Task Type                  | Colab Link |
|---------------------------|------------|
| Conversational Chat       | [Colab ğŸ”—]() | 
| Classification via Chat   | [Colab ğŸ”—]() | 
| Extend Max Context (TinyLlama) | [Colab ğŸ”—]() |

- **ğŸ¥ Video**: [Watch ğŸ¥]()

---

## d) Reward Modeling: ORPO & DPO

Used ORPO and DPO techniques to align models using preference data.

| Technique | Colab Link |
|-----------|------------|
| ORPO      | [Colab ğŸ”—]() | 
| DPO       | [Colab ğŸ”—]() | 

- **ğŸ¥ Video**: [Watch ğŸ¥]()

---

## e) Continued Finetuning from Checkpoint

Demonstrated how to resume fine-tuning from a saved checkpoint and extend training.

- **Colab Notebook**: [Link ğŸ”—]()
- **ğŸ¥ Video**: [Watch ğŸ¥]()

---

## f) Mental Health Chatbot Fine-Tuning

Fine-tuned Microsoft Phi-3 using a custom mental health dataset to develop a therapeutic chatbot.

- **Colab Notebook**: [Link ğŸ”—]()
- **ğŸ¥ Video**: [Watch ğŸ¥]()

---

## g) Export Fine-Tuned Model to Ollama

Used Unsloth to fine-tune a model and export it to Ollama for lightweight local inference.

- **Colab Notebook**: [Link ğŸ”—]()
- **ğŸ¥ Video**: [Watch ğŸ¥]()

---

## h) Reasoning Model Fine-Tuning (GRPO)

Trained a reasoning model with GRPO from scratch and evaluated it on reasoning datasets.

- **Colab Notebook**: [Link ğŸ”—]()
- **ğŸ¥ Video**: [Watch ğŸ¥]()

---

## ğŸ“Œ Notes
- All models were fine-tuned using Unsloth LoRA adapters for efficiency.
- All demos were run on Google Colab with GPU enabled.
- Each notebook is accompanied by a short 1-2 minute video walkthrough.

---
