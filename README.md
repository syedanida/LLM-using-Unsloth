# 🚀 LLM Fine-Tuning & Pretraining Assignment with Unsloth

This repository contains Colab notebooks and demo videos showcasing various advanced fine-tuning, pretraining, and deployment use cases using open-weight LLMs and the [Unsloth AI](https://unsloth.ai/) ecosystem.

---

## 📁 Contents

a. Fine-Tuning Various Open-Weight LLMs

b. Continued Pretraining

c. Chat Templates: Multi-Task Finetuning

d. Reward Modeling: ORPO & DPO

e. Continued Finetuning from Checkpoint

f. Mental Health Chatbot Fine-Tuning

g. Export to Ollama and Inference

h. Reasoning Model Fine-Tuning (GRPO)

---

## a) Fine-Tuning Various Open-Weight LLMs

Fine-tuned models on different tasks such as coding, chat, instruction-following, and classification.

| Model Used           | Use Case Type  | 
|----------------------|----------------|
| Llama 3.1 (8B)       | Coding Assistant | 
| Mistral NeMo (12B)   | Instruction Chat |
| Phi-3.5 (mini)       | Conversational Chatbot | 
| Gemma 2 (9B)         | Text Classification | 

- **🎥 Video**: [Watch 🎥](https://youtu.be/h39J8gDmQlk)

---

## b) Continued Pretraining

Using Unsloth AI, continued pretraining to teach an LLM a new language.

- **🎥 Video**: [Watch 🎥](https://youtu.be/h39J8gDmQlk)

---

## c) Chat Templates: Multi-Task Finetuning

Explored Unsloth's chat templates to perform multiple tasks in a single finetuning run.

| Task Type                  | 
|---------------------------|
| Conversational Chat       |
| Classification via Chat   | 
| Extend Max Context (TinyLlama) | 

- **🎥 Video**: [Watch 🎥](https://youtu.be/h39J8gDmQlk)

---

## d) Reward Modeling: ORPO & DPO

Used ORPO and DPO techniques to align models using preference data.

| Technique | 
|-----------|
| ORPO      | 
| DPO       | 

- **🎥 Video**: [Watch 🎥](https://youtu.be/h39J8gDmQlk)

---

## e) Continued Finetuning from Checkpoint

Demonstrated how to resume fine-tuning from a saved checkpoint and extend training.

- **🎥 Video**: [Watch 🎥](https://youtu.be/h39J8gDmQlk)

---

## f) Mental Health Chatbot Fine-Tuning

Fine-tuned Microsoft Phi-3 using a custom mental health dataset to develop a therapeutic chatbot.

- **🎥 Video**: [Watch 🎥](https://youtu.be/h39J8gDmQlk)

---

## g) Export Fine-Tuned Model to Ollama

Used Unsloth to fine-tune a model and export it to Ollama for lightweight local inference.

- **🎥 Video**: [Watch 🎥](https://youtu.be/h39J8gDmQlk)

---

## h) Reasoning Model Fine-Tuning (GRPO)

Trained a reasoning model with GRPO from scratch and evaluated it on reasoning datasets.

- **🎥 Video**: [Watch 🎥](https://youtu.be/h39J8gDmQlk)

---
