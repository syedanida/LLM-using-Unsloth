{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wcl9_noktLuh",
    "outputId": "3e6b6976-d66a-43d9-c9cc-8f1e159a5ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ollama 0.4.7 requires httpx<0.29,>=0.27, but you have httpx 0.26.0 which is incompatible.\n",
      "google-genai 1.10.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q unsloth wandb transformers torch accelerate datasets bitsandbytes sentencepiece\n",
    "!pip install -q packaging ninja\n",
    "!pip install -q ollama-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDVLC9W0wJPe",
    "outputId": "5f6ef1eb-6f35-42ef-fbd9-569e83a051e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.4.7)\n",
      "Collecting httpx<0.29,>=0.27 (from ollama)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.11.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Installing collected packages: httpx\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.26.0\n",
      "    Uninstalling httpx-0.26.0:\n",
      "      Successfully uninstalled httpx-0.26.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ollama-python 0.1.2 requires httpx<0.27.0,>=0.26.0, but you have httpx 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed httpx-0.28.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "TC6HRDdLtnXw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments\n",
    "import ollama\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "vLLXOHa_tsqA"
   },
   "outputs": [],
   "source": [
    "# Disable Triton to avoid BF16 errors on T4\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"PYTORCH_TRITON\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "KqfkIjfvtsnm"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    # Load a sample dataset\n",
    "    dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "    dataset = dataset.select(range(100))  # Small subset for demo\n",
    "\n",
    "    def format_instruction(example):\n",
    "        system_prompt = \"You are a helpful assistant.\"\n",
    "        instruction = example[\"instruction\"]\n",
    "        input_text = example[\"input\"] if example[\"input\"] else \"\"\n",
    "        output = example[\"output\"]\n",
    "\n",
    "        if input_text:\n",
    "            formatted_text = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{instruction}\\n{input_text}\\n<|assistant|>\\n{output}\"\n",
    "        else:\n",
    "            formatted_text = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{instruction}\\n<|assistant|>\\n{output}\"\n",
    "\n",
    "        return {\"text\": formatted_text}\n",
    "\n",
    "    formatted_dataset = dataset.map(format_instruction)\n",
    "    return formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "YKL6w49NtxpH"
   },
   "outputs": [],
   "source": [
    "def finetune_model():\n",
    "    print(\"Loading model...\")\n",
    "\n",
    "    # Use DistilGPT2 instead - much smaller and should work on T4\n",
    "    model_name = \"distilgpt2\"  # Only 82M parameters\n",
    "\n",
    "    # Load model using HuggingFace directly instead of Unsloth for this case\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Make sure tokenizer has padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,  # Avoid float16 or bf16 entirely\n",
    "    )\n",
    "\n",
    "    # Add LoRA using PEFT directly instead of Unsloth's wrapper\n",
    "    from peft import get_peft_model, LoraConfig\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 style attention modules\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # Prepare dataset\n",
    "    train_dataset = prepare_dataset()\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    tokenized_dataset = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "\n",
    "    # Add labels for causal LM training\n",
    "    def add_labels(examples):\n",
    "        examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "        return examples\n",
    "\n",
    "    tokenized_dataset = tokenized_dataset.map(add_labels)\n",
    "\n",
    "    # Set up training arguments - avoiding any fp16/bf16 usage\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=4,  # Can use larger batch size with smaller model\n",
    "        gradient_accumulation_steps=4,\n",
    "        optim=\"adamw_8bit\",  # Use 8-bit optimizer to save memory\n",
    "        learning_rate=5e-4,\n",
    "        weight_decay=0.01,\n",
    "        fp16=False,  # Disable fp16\n",
    "        bf16=False,  # Disable bf16\n",
    "        max_grad_norm=0.3,\n",
    "        warmup_ratio=0.03,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=5,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"wandb\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    # Set up trainer\n",
    "    from transformers import Trainer\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    output_dir = \"./fine_tuned_model\"\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    print(f\"Model saved to {output_dir}\")\n",
    "    return output_dir, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "O9AjeyBotxmY"
   },
   "outputs": [],
   "source": [
    "# Export the model to Ollama format\n",
    "def export_to_ollama(model_dir, model, tokenizer):\n",
    "    print(\"Preparing model for Ollama export...\")\n",
    "\n",
    "    # Create Ollama directory\n",
    "    ollama_dir = \"./ollama_model\"\n",
    "    os.makedirs(ollama_dir, exist_ok=True)\n",
    "\n",
    "    # Create Modelfile for Ollama - using GPT-2 base\n",
    "    modelfile_content = \"\"\"\n",
    "    FROM gpt2\n",
    "    PARAMETER temperature 0.7\n",
    "    PARAMETER top_p 0.9\n",
    "    PARAMETER top_k 40\n",
    "    \"\"\"\n",
    "\n",
    "    with open(f\"{ollama_dir}/Modelfile\", \"w\") as f:\n",
    "        f.write(modelfile_content)\n",
    "\n",
    "    # Create manifest\n",
    "    manifest = {\n",
    "        \"name\": \"fine-tuned-gpt2\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"license\": \"custom\",\n",
    "        \"architecture\": \"gpt2\",\n",
    "        \"parameters\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 40\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(f\"{ollama_dir}/manifest.json\", \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "    # Copy model files\n",
    "    shutil.copytree(model_dir, f\"{ollama_dir}/model\", dirs_exist_ok=True)\n",
    "\n",
    "    print(f\"Model prepared for Ollama at {ollama_dir}\")\n",
    "    return ollama_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "9J-INJOetxj_"
   },
   "outputs": [],
   "source": [
    "def setup_ollama_and_inference(ollama_model_dir):\n",
    "    print(\"Setting up Ollama for inference...\")\n",
    "\n",
    "    print(\"Command to create Ollama model:\")\n",
    "    print(f\"!ollama create fine-tuned-gpt2 -f {ollama_model_dir}/Modelfile\")\n",
    "\n",
    "    print(\"\\nDemo inference (would run on actual Ollama installation):\")\n",
    "    print(\"Example prompt: 'Write a short poem about machine learning'\")\n",
    "\n",
    "    print(\"\\n=== USAGE INSTRUCTIONS ===\")\n",
    "    print(\"1. Install Ollama on your machine: https://ollama.ai/download\")\n",
    "    print(\"2. Copy the model files to your Ollama directory\")\n",
    "    print(\"3. Run: ollama create fine-tuned-gpt2 -f Modelfile\")\n",
    "    print(\"4. Start inferencing: ollama run fine-tuned-gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "3IQUqYvktslA"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Starting fine-tuning and Ollama export process...\")\n",
    "    model_dir, model, tokenizer = finetune_model()\n",
    "    ollama_model_dir = export_to_ollama(model_dir, model, tokenizer)\n",
    "    setup_ollama_and_inference(ollama_model_dir)\n",
    "\n",
    "    print(\"\\nProcess complete! To use this model with Ollama:\")\n",
    "    print(\"1. Install Ollama: https://ollama.ai/download\")\n",
    "    print(\"2. Follow the usage instructions above to import and run the model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952,
     "referenced_widgets": [
      "d397bd97f3234667aa791989fb309533",
      "25603d8e1ef14a1b8cae79dc44cd099c",
      "932b406cca054000aed81c976632c74f",
      "018f0b78857d4c2995dab062cf7ff2c9",
      "486a4435268f498fae597833fdfbe355",
      "01f1610b783f4f329c94a650eb578cc9",
      "c7749853dc224bc597b0e9fd0bc64960",
      "78407365df7741158f3646046aad12a2",
      "13fae2918b4b43f593b1b6bdc7068b87",
      "ca2827535ec447c096b8c8879f89ca99",
      "ba320e35f9554238ab5b80e2f3bec534",
      "e9d3e0f0363448cc89b02081bfce2d63",
      "3d2836d47845406983f9cab2309dede8",
      "88a83b36a2bf441fadb0afe8b48c7522",
      "447d944066bc412189f764969f9ae823",
      "abb8d94137b44f26a0cb8a574d1bc96e",
      "3ea015933bb242e2b94f5d583ac1832e",
      "32b9925d73224a0fab681d2bd84d0255",
      "1610028ecaa34de1a9287fe2536a34ca",
      "9127aaa1cae84d5b82aab75023b771db",
      "4fe0b483978747bcb658bdc6877c6c40",
      "ebe3ffea64654f1e9c85faaef41ec9f1",
      "eeb515e9fae8431ab26f61273ac71bcb",
      "7147f6181c174bdd944391cfd579fcbf",
      "299eac32d240404ea95fceed245b2653",
      "7c79464de054467db173d4d271653967",
      "256a530a410e49b69bd5ae9ccbd15444",
      "725efbf2824849de84c7e02e99c599c5",
      "229289f2a63b4a4b8cb98718c1995c24",
      "0508774c4f8845fe8e502107cfd07afa",
      "fbec96c309ce4618b0a13ab525d2c4e4",
      "bad14afa1a494efea03fd95922d3da1d",
      "45b0d32651494bb59b4374451691b773",
      "58c7dcbd69ae451d8f8d56f90ff0d7a6",
      "e2a756fb39f446aead9e99721e8306ae",
      "a0a05476089940f78dda2722d12235d2",
      "ce7d5ecde23247ce86572d33c01dd976",
      "69184b2e99104d5c86e60f42c6cdc95f",
      "3546d2ac76e64d16b4864bd919ae3272",
      "4689d03f4caa448d90faea6198afec88",
      "a03cff206152463a9df326a18f7ad385",
      "6c401d6ae99b4f4493d767829f47678c",
      "ff209dfa08584419959d9b8d9b0cb2c5",
      "7c54e79e55f642d89ef831af77f57415",
      "87309341e9104afc8dcb81bcf8bc4600",
      "1549c5f24fcc4ea3a1fd17a25ecff09b",
      "36d06dd2c3bb414593562421076e6543",
      "7a35f106b99341738ac275e0fe6ecb09",
      "7d20f1bb9b8e443f946a52822b6ef6f1",
      "ae37f04970e94af58358fc273bde3607",
      "9e10fca3e44a4873b16c1dc60f982864",
      "1f3829c8f918445a894861f560cc668d",
      "6b25597bf2f9472e894cd44488dcee35",
      "d9ec0dd76b804e7e8495c77e52bc6d03",
      "d0c61ee547fe47d792a39d8b162e6d27",
      "6128991a66714b2cbca68c3ed52871a2",
      "9026a386690a484fb25b1beeea14864e",
      "be5ef685da314e6aa9b3f915cdbce062",
      "446f00003764480fafb5bf381b9a9914",
      "d2041b2986c94423afcfb0e321f929e6",
      "bd47a17098384bbb98e64abc47b23e5b",
      "a4aa16f01cee4d4caf66102d1c0b35cb",
      "8deae4875c2e4b1ead340f3c0cfd02f9",
      "ae58208fea4d453aa95595c778d6dfef",
      "74c6aeba26af46f699144976b03e8aee",
      "c53a5b174d7a4a4285ce136e0accbe29",
      "c7f72c21395a442d9f41f2a4ec241966",
      "61c9f0891dbd46e28b12090873ad49c9",
      "22a7bb5788e34784900dc0223607d864",
      "1337f200a570464eb25662d5751cc866",
      "7b4bd3c4cacc49df96a52887d8894070",
      "37e4e3a8603d4448a8c95550a23257ed",
      "bf6e439a9cac498397eeabce8bf9835d",
      "7905d6020aca4938803b97f9a16c33cf",
      "398646a0b9984c48b0df09f6415c55f9",
      "1c13ca0a508a4b3184af19fba949d106",
      "b144a32b455c45f8a07fe17ddae89b9e",
      "51c79520710f45359fe4274cc1e85707",
      "f724fd26a1eb43cc93f51c1a99b3b687",
      "87fd83d0c9de4f2ebb07f5372a6f2ecd",
      "c16c407594684e4285de87f3c5d238c6",
      "e4671c7b42cb464591a7d57b50a2b7bd",
      "a68eb30607704d498ecd4b14bb444d9a",
      "028ba1e9292d42a0a6bbf67a98e16eec",
      "482916a0823a454981b8860731e326ae",
      "8d495916fd464001b20128788786e2fb",
      "94248fe23701466ca07666a1fe6437e5",
      "e13ed36d1abf4c25a25c0c2f5a0cdf05",
      "8610e742d7c1478aae1f58c84ad053c0",
      "2e4760cf0c6346888568d58291d51ae0",
      "e14e07ee41a348a18b9dc585097f709a",
      "f8d5edf161e043ba8d7f5d85a9c21480",
      "d4ac492852314a86be8ea4511913e1f9",
      "0c4f11fa25784434bc8ef67c78616021",
      "58e641b0de904782928db136a8b71834",
      "cf710df63f08410a99f92f7e0c3d76c7",
      "36845deb1104475db6356bdd80bcd594",
      "178b0aba3ac542e5843b9ef25d470de9",
      "5e53d0ae3409493aa33c6b729b273845"
     ]
    },
    "id": "9yz7eccztnUx",
    "outputId": "903fa4b6-ac99-4d20-f09d-4fa14dafcecf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning and Ollama export process...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d397bd97f3234667aa791989fb309533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d3e0f0363448cc89b02081bfce2d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb515e9fae8431ab26f61273ac71bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c7dcbd69ae451d8f8d56f90ff0d7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87309341e9104afc8dcb81bcf8bc4600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6128991a66714b2cbca68c3ed52871a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f72c21395a442d9f41f2a4ec241966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c79520710f45359fe4274cc1e85707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8610e742d7c1478aae1f58c84ad053c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 1 | Total steps = 6\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 202,752/82,115,328 (0.25% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>9.403500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./fine_tuned_model\n",
      "Preparing model for Ollama export...\n",
      "Model prepared for Ollama at ./ollama_model\n",
      "Setting up Ollama for inference...\n",
      "Command to create Ollama model:\n",
      "!ollama create fine-tuned-gpt2 -f ./ollama_model/Modelfile\n",
      "\n",
      "Demo inference (would run on actual Ollama installation):\n",
      "Example prompt: 'Write a short poem about machine learning'\n",
      "\n",
      "=== USAGE INSTRUCTIONS ===\n",
      "1. Install Ollama on your machine: https://ollama.ai/download\n",
      "2. Copy the model files to your Ollama directory\n",
      "3. Run: ollama create fine-tuned-gpt2 -f Modelfile\n",
      "4. Start inferencing: ollama run fine-tuned-gpt2\n",
      "\n",
      "Process complete! To use this model with Ollama:\n",
      "1. Install Ollama: https://ollama.ai/download\n",
      "2. Follow the usage instructions above to import and run the model\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
