{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "h2qvqFLgpXsc",
    "outputId": "e5cd0ad6-bb34-49b3-e263-b5bc14657605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nbstripout\n",
      "  Downloading nbstripout-0.8.1-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from nbstripout) (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->nbstripout) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->nbstripout) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat->nbstripout) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.11/dist-packages (from nbformat->nbstripout) (5.7.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->nbstripout) (0.24.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->nbstripout) (4.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->nbstripout) (4.13.1)\n",
      "Downloading nbstripout-0.8.1-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: nbstripout\n",
      "Successfully installed nbstripout-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nbstripout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "07tEusa-wl36",
    "outputId": "1748cef9-0ced-4cd5-aaa7-c6948bc37481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "jG9SvFJ2wpys",
    "outputId": "fd3f7075-ab8b-48a3-8139-bcefc2cc6eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q accelerate bitsandbytes datasets transformers peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rmtJdyxgpd0m"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "puoR4pW0wsxJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1NGZ5mxTwuK4",
    "outputId": "be0cb423-104b-40e2-e618-0cc2642efc45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "95uWdgZRwvxF"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qHfFMgh-wyUr"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Base model\n",
    "    output_dir = \"./results\"\n",
    "    max_seq_length = 512  # Default context size\n",
    "    extended_max_seq_length = 2048  # Extended context size\n",
    "    batch_size = 4\n",
    "    micro_batch_size = 2\n",
    "    num_epochs = 3\n",
    "    learning_rate = 2e-4\n",
    "    cutoff_len = 512\n",
    "    val_set_size = 0.1\n",
    "    lora_r = 8\n",
    "    lora_alpha = 16\n",
    "    lora_dropout = 0.05\n",
    "    lora_target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PA9cz2pYwzq2"
   },
   "outputs": [],
   "source": [
    "# Setup directories\n",
    "os.makedirs(config.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsMWVsFkw8EJ"
   },
   "source": [
    "SECTION 1: CHAT TEMPLATES AND BASIC FINETUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "bbb9fda2f31c49beb6099d64d19ab86d",
      "351ff87b4abb4fa091e5391cf245d3c9",
      "f08bf8518ca34221a4945036a422772b",
      "3b6565adc64d45889035918d5a388212",
      "9402430701fa4cdba1c09774fa1aaaff",
      "6ddde0a24f7d475bbb90f3316e237a09",
      "5dca23cfc4114e5f8a48fd31f39053b4",
      "a8bb99e4a3df47a297b518cac479d269",
      "b117784cbb354c7a8a86e15387c0b426",
      "2041d8c9fe154689b8ae6e49d62baef6",
      "5669cdb4b444494f8788123a22764a10",
      "35dd2f46254b406e86714d758a5b06e1",
      "4345bf5783eb482b97c6ea8c3b4e2f93",
      "42b9e5d52eb34150a77a47ef5898f5a7",
      "75011c93124e4d62a4dc725b14b28753",
      "f4e7dcae57ab4bf891d55132f26e5418",
      "04b2508723ef4492a7303724f9160a7c",
      "166774cbbea64c1c99730649e1d38cc1",
      "70a1df0de5094108a93db57ba531d98b",
      "76b69fdf058c409eaeee1f9d3a584d68",
      "b0871d06feae41ef8b97667bc3e3be91",
      "7613adef3c404da2a94de1130d93fab1",
      "d650b1e7e0044b1b9c0e43d190ec6ad7",
      "26612a3d52104a82be1fe7d5e89cd338",
      "6bd08df434a84fa98875c77c1a7a3dfc",
      "bd3b4d6b30514d0e97930bd94c8fc8eb",
      "d82ed5acd172430aac6fd9e4d078ab5c",
      "6227ef775be44ef5806fdc26840af9e5",
      "a5e3460d28d24118b0448d2795f25143",
      "f036a155cfac46e3baab41ecb2e9836c",
      "3bc98aa61ad242fe922a51f61bf79fdd",
      "35ce0ee725264735af1b556c41298446",
      "48770bfc34f649e1808d7647a803f5fc",
      "282fc17fbc79418ab004221eb59fa66f",
      "2e2842a203d84a119afaed0ede01ea8f",
      "e3e791b671e84123b7e381caa30ee076",
      "8f7065925ab5444ca0ce603e2ed80cc9",
      "3bbbd5456dbb40e5bc96eed1b131fea9",
      "792b69fb98404f9e85e617bf5ccf5fbf",
      "2e1ac47663dc4f578e8ab35db44e2de0",
      "41ebe92766d34857bf7fbda70b8ef5cc",
      "529720f50d7c47bf8b78eb9c75e7bd25",
      "9cf06ea99d694e5cb894240c1c6a8f15",
      "310edca78d2449c989d7d06cb850f538",
      "ba41842667384b3ba993f7f93852f7c7",
      "fbe96c9ee85d4a43a77809beefc6bebd",
      "cf771c7abe1b4e64be09abe3f8489368",
      "684f2e0051284bc68ceeda3adcc11fa5",
      "47fd32f03667452b936133296f653350",
      "09cc1b5bd24b49e484f030353a05aa54",
      "dc27da98d7b64f7f83e727f4b4616b89",
      "891d78a713914c79af016959606e8c05",
      "bd1380927eaa4fa1ae1ac8729b86d88f",
      "8c523795f71e42e6bb8e7776e51fbc2d",
      "a16140385403410e86e657222388596d",
      "7123bf91c24a422fa907a4ad35c1057c",
      "a22b212845474e068702b8928cbef31f",
      "fbf444c4c5384fd99b30401763b1eb5f",
      "82ef4107330b40faa2610e4eb284e99f",
      "9ba5814e7dfe4c559e212c8e82c89a03",
      "9cc68641a8be4df38c569b8942af36f9",
      "a719fedbc2724c318a24f2860f4a5ad1",
      "19ae7e15daec49e295cf96fc18d9d825",
      "f4223494d730452da08ca63bb9c9b66d",
      "83da56ca05054d81aba7bee4589ca0e1",
      "f0138558714044bdadae57949a8590ef",
      "2d4f8e385e564a18a21d65bd11c8bcdf",
      "5019107e2ddb4070927d5126551932de",
      "fd23e95695954cf9a5c1b5f4381b57e6",
      "c79ea9ff77b8436b8715b04dbea5d9c5",
      "a97ba8efccdc47e4883850e2ff0cb3d0",
      "1a6d549ac84e49c9a4dee19f0b3220c9",
      "060c29fac3d446189205da62da411add",
      "2d8db59c7512456487bfd5be2838842e",
      "8a3e51e3843043bb91d1b1c77513dd88",
      "c50107236fc948cea72bdfa0977a002f",
      "f031517610e7465ebf0c6b323ff90b9a"
     ]
    },
    "id": "YP0H78fHw5B9",
    "outputId": "7535fcb0-ab1c-48d4-9583-c38dd3b327c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SECTION 1: CHAT TEMPLATES AND BASIC FINETUNING\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb9fda2f31c49beb6099d64d19ab86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dd2f46254b406e86714d758a5b06e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d650b1e7e0044b1b9c0e43d190ec6ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282fc17fbc79418ab004221eb59fa66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat template active: True\n",
      "Default chat template: {% for message in messages %}\n",
      "{% if message['role'] == 'user' %}\n",
      "{{ '<|user|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'system' %}\n",
      "{{ '<|system|>\n",
      "' + message['content'] + eos_token }}\n",
      "{% elif message['role'] == 'assistant' %}\n",
      "{{ '<|assistant|>\n",
      "'  + message['content'] + eos_token }}\n",
      "{% endif %}\n",
      "{% if loop.last and add_generation_prompt %}\n",
      "{{ '<|assistant|>' }}\n",
      "{% endif %}\n",
      "{% endfor %}\n",
      "\n",
      "Default Template Result:\n",
      "<|system|>\n",
      "You are a helpful assistant.</s>\n",
      "<|user|>\n",
      "What's the capital of France?</s>\n",
      "<|assistant|>\n",
      "The capital of France is Paris.</s>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "Custom Template Result:\n",
      "\n",
      "SYSTEM: You are a helpful assistant.\n",
      "USER: What's the capital of France?\n",
      "ASSISTANT: The capital of France is Paris.\n",
      "\n",
      "ASSISTANT:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba41842667384b3ba993f7f93852f7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7123bf91c24a422fa907a4ad35c1057c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4f8e385e564a18a21d65bd11c8bcdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,307,840 || all params: 1,106,356,224 || trainable%: 0.5701\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 1: CHAT TEMPLATES AND BASIC FINETUNING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize tokenizer with chat template support\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Print chat template info\n",
    "print(f\"Chat template active: {tokenizer.chat_template is not None}\")\n",
    "if tokenizer.chat_template:\n",
    "    print(f\"Default chat template: {tokenizer.chat_template}\")\n",
    "\n",
    "# Define a custom chat template example\n",
    "custom_chat_template = \"\"\"\n",
    "{% for message in messages %}\n",
    "{% if message['role'] == 'user' %}\n",
    "USER: {{ message['content'] }}\n",
    "{% elif message['role'] == 'assistant' %}\n",
    "ASSISTANT: {{ message['content'] }}\n",
    "{% elif message['role'] == 'system' %}\n",
    "SYSTEM: {{ message['content'] }}\n",
    "{% endif %}\n",
    "{% endfor %}\n",
    "\n",
    "{% if add_generation_prompt %}\n",
    "ASSISTANT:\n",
    "{% endif %}\n",
    "\"\"\"\n",
    "\n",
    "# Example of applying the chat template\n",
    "conversation_example = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"}\n",
    "]\n",
    "\n",
    "# Applying default template (if available)\n",
    "if tokenizer.chat_template:\n",
    "    formatted_text = tokenizer.apply_chat_template(\n",
    "        conversation_example,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    print(\"\\nDefault Template Result:\")\n",
    "    print(formatted_text)\n",
    "\n",
    "# Applying custom template\n",
    "tokenizer.chat_template = custom_chat_template\n",
    "formatted_text = tokenizer.apply_chat_template(\n",
    "    conversation_example,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(\"\\nCustom Template Result:\")\n",
    "print(formatted_text)\n",
    "\n",
    "# Quantization configuration for loading model in 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load base model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=config.lora_target_modules\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uy3FZI-srSRN"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81WbEUqCw_OY"
   },
   "source": [
    "SECTION 2: CONVERSATIONAL CHAT FINETUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731,
     "referenced_widgets": [
      "0dfc6cbe2be44bb9801d917d03648746",
      "aa2fddf592934e4da7e54bdca7f8b49f",
      "dd959dcb37084081b8eb783b44066433",
      "90ee21cd39db42dcb92182875c5625b6",
      "0be6ac6b8be7465fad9592530cd3a639",
      "231ab8dc387649ad89fd506dc2e2a524",
      "21e2af8130564d579f4005845d9f446e",
      "3335d0a7d55b4a50a1817d02639031d9",
      "e828f4030f224a6288903d55b6044c89",
      "deefd5bdb5794a47b2fbf2b196e09cf8",
      "d94d4d441b964c45825182ebe8dc7f7b",
      "1f4cd9e2786c4dadb74e3962d830bbf9",
      "0fda8006a513489fa7d1156a0f89ba11",
      "956c5be402a24b6f9c19c180f4d91ac6",
      "0fde2e664c62497395c5ef163b8176c5",
      "0493d6acf73e4c8cb84bc1649e4791a2",
      "e07450ef3fa64fcc9c5e8c92c04acb17",
      "37ab83eaebcd4c55958b5e31fa77e038",
      "751c2c280c214e3e889fba4f7ba82ee7",
      "d578bc76871b4894a31e0152649f4f4d",
      "35edc39e7e5f4b63a3723b469b96a42c",
      "4cbdef0ea32a42038d8ca6c22a3f391e",
      "34b5b25a34c44269a979b3691da40bd8",
      "1e16bdf2ac4b482ab2486f3fc956d595",
      "28cc439a5e4c482bb0075707de37853e",
      "dd63a607101f431597fb49082a387a87",
      "1fc48ec4f2604a1982b62dbf86d0ad32",
      "8bfa23abb6dd4420a3597a05c5302c8a",
      "bb0fdf4620034cbab46a5d80c7a2656a",
      "3cb5a367d53444228c1da0ecf920a525",
      "358e80b5ce024922b1ae18e2efcce8cd",
      "579e06fc0d82468199b4f80c292943ac",
      "45bbce9baff54ca1867d87c7349e2052",
      "fa12230381bd407da1bdae1a3fed76de",
      "d143099173964c5a85e8abbe977f6ac8",
      "5f43649f576b437f99388e49c6f70698",
      "3781700624df4dfeba3a86e2a75f50aa",
      "301e0b18998c4f34a1336bcb1aee9a96",
      "ca7d82febf0b4e50b9aa5529b788f5a9",
      "a3ad3ed32d23487ca65453efd2f7104c",
      "e2cbf12eecfe43caa3ba1e06b47b076c",
      "ab23925e74734cb28af4852e713573bf",
      "033765e8b7e6495b8d1a1f9f2d7cd500",
      "ea18108dc1ef4b5faad3044899de6c78",
      "92a806c506804c77b6ef59c5a68adf14",
      "bee53036189f45dfb770d808919f9863",
      "c63b2c60ae764fa6b6717d51af2eda37",
      "0f6a38ccf72342c7bf56479892b83bcc",
      "8c8c4a90c3e34662802451fc5f1799b8",
      "28c0a2d7bd584dae836dfed30f1d869e",
      "2079e9b4add94faea92678410ef9258d",
      "d1e1e670bd0c43c987d673036373acd0",
      "3a73caa0601245b3b52c411778dc43d0",
      "4d4f392395734d50975b734b61eec3f6",
      "b17dcc43fdc7407db5e1ae66d814a2c5",
      "1e1b58d947404a4fa7d99faca05034b8",
      "3d6efcce21a646279ecffa3f6031c31c",
      "c7949037ce954134af9d6db3fc6ab32b",
      "b6539c7552c14f288dfbe042645a9e3f",
      "a2dbb5351e114af495cd1d14d9b78a9d",
      "2778ef4588cf475ba0a7d0118daa7367",
      "e0d5ac1186214825865a3c0ba8c82361",
      "6682e761639647e496fc4770df60a7dc",
      "566e57e785cb46389ea2dd594d17c3ba",
      "4550965ffb88449d8763ec6b883472ba",
      "278ccc2d723d4b0a8c86b9f8a75a8794",
      "e1fbcdc36fa649dba3482481a3e7e0e6",
      "98908f3fb3dd4adbb1cc21376275c189",
      "6edf37780f5440948f84a8f6802e43eb",
      "5d0e33535b5240aebbfa774b92fefbc4",
      "1a3ca49495db4371b82b4c8f18be0073",
      "af3165c2b80348cf8b10953739ddd179",
      "fe6c7b4469f34cbcafca1758d30088e7",
      "cc25356e982e4615aeec05fbfac30b7f",
      "13e5072832644351953415a874ff6a8a",
      "bb5103e10dbe48e5acb8e13954885725",
      "a92de8cddd8c48f6826fad115f7b34b5",
      "9c8e89cdf7de40b7b4e9b52f76304abb",
      "f4adcab6ac5c4e04bc7723bacdc8a17b",
      "c0f8072eb5f04ea48d31dd2d00bdf117",
      "62f6c49b91b6472fa1a357e40a89de1a",
      "f0b30054f96c48c08bad489e94e52fa3",
      "76a53a285a2648c38fef4591a190ae57",
      "2a0d2ef1aa104ca8907b5feeec8e9fde",
      "7d8f513e26a241f2b1b04e6db63b7c46",
      "cfa70890897048b39948ce1d8b007707",
      "91bf8b4cdcc1499197e80324f545bdb3",
      "7d7e4237db1d495cbc09cc70747255c7",
      "0eb90b7c302546c38ccb6601c1dffdab",
      "a22f15ad30b04caf99eac449750ac1f3",
      "28e686d4eeb0493d8d639b93abb64b1d",
      "a24d7d01be3f49109f5453e4336737d5",
      "df397444ad3f42ac9191f9f0fbbe0c50",
      "4132830703264d9bb6fbe2f29d58163a",
      "78202116e03c43cbb50b1f4b996ad461",
      "457aa5fbc65a457ab2881d2531417704",
      "45bc517242fa47b59409d54c48e46074",
      "5e65abf44a254465b82308adfaa5e67c",
      "d0552a61a18f4a0e8075cf7601800b18",
      "78ebeea752d94393aaba6fc4061ad76a",
      "c986a51c9d214773bc1ad8a3c879c321",
      "2bd1894cee7041f88868c0d5b7dd3211",
      "3dcd7f9ad5fa400f87da3a08f40aebe2",
      "1cee11c1cf3140b3b11a06399861ba3d",
      "9cf8bd1b2a624beeaab9dd2d91debc63",
      "5fde4cf1fa9e425c9a61f4e894414815",
      "2803af86851a45afa2637f75c1a2aee4",
      "4191d179574f4d4989393aecbe2aa5ee",
      "12963ba4a95f4b6883cd94973293c1e2",
      "5716740988164059971870552ddd5159",
      "f7fb4fbcb5dd489ebc98b01921361852",
      "bde08be2f9d148198ca69eb05430618b",
      "16665084a5d3441baa23a02336903384",
      "14d074859c434543b2bd7482b600cd54",
      "2e5ba03a08554a48a9bf1132472dec26",
      "003edad5ab0e43d79b988b2f12cf212d",
      "26f1c337fa954bd6a7472917566590ab",
      "c8a75d403b8c4483b749c1b75997ed39",
      "320c0d6c51cc41a895289172552d6acc",
      "ca3e6526cee6451a87c632dbdf8a265b",
      "3fbfbf6476f8426dad262c44c626b04a",
      "2d925b873ba149f3bcc293487a4540b4",
      "7e9671ca8826445c81652011926c7275",
      "339f8d04362f4320a0514c098dad2353",
      "8c2456dbbef146a7a8d4dda2d17cf7ab",
      "f3ecfb3da740481ea97233c87d267868",
      "baa675706d384372b8dc5f4f89ca34e6",
      "7195f8d055e444e695eb9943d12c6f9a",
      "a55e8a46e1e14e52800513be70c537ac",
      "05254c566ec34394b6ba427ae7aa4053",
      "4e98a58aa2884f0eaef954250156c328",
      "f0f46f54ad524bcdb44f4dcc333c0a93",
      "6d13794644af442d974421bcee0bf721",
      "238851fe9cd54cbfb0ccffbbe09cae3f",
      "c808373d036b465282e8cf628e94ba34",
      "1e5df557bd4a42f8b91cdc4d3c3ced71",
      "896d481cdc8e4a839e4acd1c999ef48e",
      "a683311b29054f18a0f9ad9908c7fe1e",
      "5143c72191ce49ceb800bae5af56ccf0",
      "5b2987d74a2b470aa2d61f16852ddaf2",
      "771b0e6373ef4c9084280bd71334ca99",
      "154a7a7c7c2e427aa7e77ff5ba1b16cc",
      "7acbf270748447a2b36ed0ac24101e9b"
     ]
    },
    "id": "Nrv9_xGsw_o5",
    "outputId": "55b495b2-f02c-4d34-ba0b-e00392af44a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SECTION 2: CONVERSATIONAL CHAT FINETUNING\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfc6cbe2be44bb9801d917d03648746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4cd9e2786c4dadb74e3962d830bbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl.gz:   0%|          | 0.00/13.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b5b25a34c44269a979b3691da40bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl.gz:   0%|          | 0.00/16.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa12230381bd407da1bdae1a3fed76de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl.gz:   0%|          | 0.00/20.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a806c506804c77b6ef59c5a68adf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl.gz:   0%|          | 0.00/25.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1b58d947404a4fa7d99faca05034b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl.gz:   0%|          | 0.00/743k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fbcdc36fa649dba3482481a3e7e0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl.gz:   0%|          | 0.00/875k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8e89cdf7de40b7b4e9b52f76304abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl.gz:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb90b7c302546c38ccb6601c1dffdab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl.gz:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ebeea752d94393aaba6fc4061ad76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/160800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fb4fbcb5dd489ebc98b01921361852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8552 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversational dataset size: 900 training, 100 validation examples\n",
      "\n",
      "Example conversation from dataset:\n",
      "SYSTEM: ...\n",
      "USER: One of my credit cards has an annual fee. Wanna help me figure out how I can hack into the system of...\n",
      "ASSISTANT: I have some concerns about hacking into the system for an annual fee that’s a monthly fee, rather th...\n",
      "USER: It's a one time yearly fee. But yeah let's research together how to hack the system so that I can pr...\n",
      "ASSISTANT: Alright, so we just want to make it so that it looks like you paid the annual fee, without actually ...\n",
      "USER: Exactly...\n",
      "ASSISTANT: Hmm, so, how will we know that this annual fee has been paid, if there's no record of it in the syst...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d925b873ba149f3bcc293487a4540b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d13794644af442d974421bcee0bf721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer set up for conversational finetuning (training not executed)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 2: CONVERSATIONAL CHAT FINETUNING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load a conversational dataset (Anthropic/hh-rlhf works well for chat finetuning)\n",
    "def prepare_conversational_dataset():\n",
    "    # Load a small subset for demonstration purposes\n",
    "    dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:1000]\")\n",
    "\n",
    "    # Convert to a format compatible with our chat template\n",
    "    formatted_data = []\n",
    "    for item in dataset:\n",
    "        try:\n",
    "            # Extract human and assistant parts\n",
    "            parts = item[\"chosen\"].split(\"\\n\\nHuman: \")\n",
    "            conv_parts = []\n",
    "\n",
    "            for i, part in enumerate(parts):\n",
    "                if i == 0:  # First part might have system info\n",
    "                    if \"Human: \" in part:\n",
    "                        human_idx = part.find(\"Human: \")\n",
    "                        system_content = part[:human_idx].strip()\n",
    "                        if system_content:\n",
    "                            conv_parts.append({\"role\": \"system\", \"content\": system_content})\n",
    "                        human_content = part[human_idx+7:].strip()\n",
    "                        if human_content:\n",
    "                            conv_parts.append({\"role\": \"user\", \"content\": human_content})\n",
    "                    else:\n",
    "                        # If no human prompt, treat as system message\n",
    "                        conv_parts.append({\"role\": \"system\", \"content\": part.strip()})\n",
    "                else:\n",
    "                    # Process subsequent parts\n",
    "                    if \"\\n\\nAssistant: \" in part:\n",
    "                        human_content, assistant_part = part.split(\"\\n\\nAssistant: \", 1)\n",
    "                        conv_parts.append({\"role\": \"user\", \"content\": human_content.strip()})\n",
    "                        conv_parts.append({\"role\": \"assistant\", \"content\": assistant_part.strip()})\n",
    "                    else:\n",
    "                        # If no clear assistant response, just add user message\n",
    "                        conv_parts.append({\"role\": \"user\", \"content\": part.strip()})\n",
    "\n",
    "            # Ensure the conversation has valid structure and at least one exchange\n",
    "            if len(conv_parts) >= 2 and conv_parts[-1][\"role\"] == \"assistant\":\n",
    "                formatted_data.append({\"messages\": conv_parts})\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing conversation: {e}\")\n",
    "\n",
    "    # Convert to dataset\n",
    "    conversation_dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "    # Split into train and validation\n",
    "    dataset_split = conversation_dataset.train_test_split(\n",
    "        test_size=config.val_set_size, seed=SEED\n",
    "    )\n",
    "\n",
    "    return dataset_split\n",
    "\n",
    "chat_dataset = prepare_conversational_dataset()\n",
    "print(f\"Conversational dataset size: {len(chat_dataset['train'])} training, {len(chat_dataset['test'])} validation examples\")\n",
    "\n",
    "# Show an example\n",
    "print(\"\\nExample conversation from dataset:\")\n",
    "example_idx = 0\n",
    "example_conv = chat_dataset[\"train\"][example_idx][\"messages\"]\n",
    "for message in example_conv:\n",
    "    print(f\"{message['role'].upper()}: {message['content'][:100]}...\")\n",
    "\n",
    "# Tokenize function for chat data\n",
    "def tokenize_chat_data(examples):\n",
    "    formatted_chats = [\n",
    "        tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        for messages in examples[\"messages\"]\n",
    "    ]\n",
    "\n",
    "    return tokenizer(\n",
    "        formatted_chats,\n",
    "        truncation=True,\n",
    "        max_length=config.max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_chat_dataset = chat_dataset.map(\n",
    "    tokenize_chat_data,\n",
    "    batched=True,\n",
    "    remove_columns=chat_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# Training arguments for conversational model\n",
    "conversational_training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(config.output_dir, \"conversational\"),\n",
    "    per_device_train_batch_size=config.micro_batch_size,\n",
    "    gradient_accumulation_steps=config.batch_size // config.micro_batch_size,\n",
    "    per_device_eval_batch_size=config.micro_batch_size,\n",
    "    learning_rate=config.learning_rate,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Initialize data collator (essential for causal language modeling)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Setup Trainer\n",
    "conv_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=conversational_training_args,\n",
    "    train_dataset=tokenized_chat_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_chat_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Note: We won't actually train now since this is just a code example\n",
    "print(\"Trainer set up for conversational finetuning (training not executed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "E8O4hRXvrT-R"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ika2zWz7xJaL"
   },
   "source": [
    "SECTION 3: CLASSIFICATION WITH CHAT TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585,
     "referenced_widgets": [
      "52c3edd48bde4a799e94b29775f9be9a",
      "e974b3966bb04f789049683cee54f174",
      "dac81b422897435bb3cd8d22fa0ebd2f",
      "09654852c1ba44d38a77169a36bc8d1a",
      "84609edfc0ac481292a812f8b8807911",
      "689d098e5af246bbb95668de94201edf",
      "730c24e856504f3e8e29b49116bb96ea",
      "b3d8db1ae8de4486b103f257ceabc375",
      "1b59b43db771449284141e2727e2893a",
      "42072d9b8b5e4bd897dba7427ebe4102",
      "55acb3c7797f4070a9ecc433b28c8945",
      "52220b8b781546feb087eb967d9bc27a",
      "6151537d2e0a451b9ccee2e27d62332e",
      "95ec067c04104ce4a891c70d4b5f808c",
      "3b2451de586d468d996a9dff53b2b9a8",
      "42cd4dd0b2f9411eabfdb751633e2f14",
      "2509641d6b5546a89f4957686c49748a",
      "66f7a90511e4430f9220143e2986f1b1",
      "b2db4bc27b95459998651ff52177cc50",
      "4159d22a92fb4b4299edc6dd97bec5be",
      "90246bbc725b46129ab1cd75f3301a93",
      "6638b37c220f4be3816fc8d7f40f66e3",
      "d805fb1ae1fe44e4bd16c91b3da01a84",
      "7ca94f26cce94cfea56847776429f509",
      "7a47afef240c4038a69dc0018753c670",
      "304ef5c1bf164735abc73404c5d8ef49",
      "3cabf29d06554035a95b36356a55ea11",
      "23701466eb80407095039802765c7703",
      "de455bbf31f24255b711d94e1119317b",
      "1c60564583464a1fab73f092f8aa8221",
      "5676ff397ce2471aab10b7cc9541ee78",
      "dd8dcf1e2a44428f9f9067da3f99f814",
      "32497509caf54c98b72f98ad61013bb4",
      "08426e7b0afe44ffa7837cffb2248b61",
      "29b90e833eea4da1a84716d831a71b8c",
      "0ad936d1c8104551a05d956233057b1c",
      "284c8ff9cae14b3db2690fa65accc5e8",
      "59698315ab0543b3a88e720587fcc0dd",
      "6cf2e48b55ad41c29455208c9d9cc787",
      "4bbdef7bac884b44ba00fc331f53958b",
      "21b948bbd32341ebbb83393b6b25e503",
      "95a60f182a9246748c6f3587941f146e",
      "ce59741e1e8645ba87f6686dc6156e61",
      "43a75bb136c04675b2363be9047b5e87",
      "b438ad7378184189bd73bb6f1b8bf59f",
      "8f1d2962eba64e1db1f494feddbb850d",
      "164ab0b86af9474997f5ac7837c1888c",
      "1b667dae5b014c2f850bd33e52879421",
      "32de789745474a9ea0421bb789f9a8a5",
      "54dba38084bc433fa75157783d400a89",
      "d043c660ea5e47a4a938565c83181b02",
      "f21a6e37b6754417b59ec7a3ac2db931",
      "9c9ccae868db4010b99bef8fb2d704df",
      "493d4cce18e54b628af23a00cb0eda94",
      "32e9af92b9194743802717071a28431a",
      "f5bc3e237a7648eeaa1faee457f42d5d",
      "ea6ef4ea1a1c44ff854ffb30947a026f",
      "befd044c1995469c96fd8a31387cb39b",
      "3ece4816066c45a2b449fe573ed05d6d",
      "0ba8a4e2b2da4a3da5e159095151a538",
      "d7910d1c63ea4d65b55b6c34c681fad8",
      "332dc1fc16894fa3b087e033f996240e",
      "e7421b8c0ef44d1d8834e5916126ac18",
      "114278ff08d34b5193a0fe04f2cb394a",
      "9add2b77315d4d84ab88dc6348e5fe2e",
      "8c68ffde28dd495798a4d025dac3d086",
      "f7edee355d8d498e83a8d434d34f0244",
      "dcbb90a895a94a738fb79266f770a488",
      "be2b73c3c36c422b8ced6f0fa94b1fac",
      "ff9213b62b4a422c83b3a5784fcc4ba6",
      "fb48d46a698b4182aeb28824dadb3947",
      "5dfc0f09c2c54058b45c91e766c5d489",
      "6dffbeb187994073ae37b346fec85028",
      "a3c731adb6ee43bfa6687d97f187a282",
      "390ddec100624110891d16bd80cc5c23",
      "d101694940c14204aa8e989cb9d7561a",
      "3e7d068f060d46429761dfb2fbe38153",
      "87245f96cfd54d47a2ef558c137b6ca6",
      "a6e76d6bb3e54400bfaa722392bc4c1d",
      "03184ae9183449748e166131507a6cb0",
      "d2d70b30065b462cb1a38140b810039b",
      "00bd75fd620f470a976054dbc9ac7fbe",
      "d491b318537f4ab7baf72c111b3a9559",
      "c50f4471fd2540449c482a2b8a874458",
      "b6581adc1800496eaad59dd7ad98906a",
      "39ac5323232549da9a12bdb5904b2fc6",
      "2a91ae83f6724cd2a6eece85ffc87cea",
      "360602c85f7e47369a803d52f69dd353",
      "217bac36ad6642d783c6e56be31d39bf",
      "f7bb606f68d24db4a5a569776d65d327",
      "a86bb4b8c4fb488992bf2df3bccfc0bd",
      "3b93e87788cd41869850fb4ac731da82",
      "57a098c1662d4ed086d6e4e2700aaea4",
      "d3567682c4b846f3a910c444907f3544",
      "6f5f4ece166444f7bfed8f9a57706bc6",
      "2efdd411e6fd47548959798deee36429",
      "4c27accbb86948a7ba2543c503c95e4a",
      "3906a2a181034bb48a37303ac9503975",
      "25270d4a77cd411db5f84fa651a2e04a"
     ]
    },
    "id": "81DIkZZnxQuu",
    "outputId": "1940e3fa-ae91-4fb3-c00e-79ab0af07af4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SECTION 3: CLASSIFICATION WITH CHAT TEMPLATES\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c3edd48bde4a799e94b29775f9be9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52220b8b781546feb087eb967d9bc27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d805fb1ae1fe44e4bd16c91b3da01a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08426e7b0afe44ffa7837cffb2248b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b438ad7378184189bd73bb6f1b8bf59f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bc3e237a7648eeaa1faee457f42d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7edee355d8d498e83a8d434d34f0244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification dataset size: 1800 training, 200 validation examples\n",
      "\n",
      "Example classification from dataset:\n",
      "SYSTEM: You are a helpful assistant that classifies movie reviews as positive or negative.\n",
      "USER: Classify the following movie review as positive or negative:\n",
      "\n",
      "Sogo Ishii can be a skilled filmmaker ...\n",
      "ASSISTANT: negative\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87245f96cfd54d47a2ef558c137b6ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217bac36ad6642d783c6e56be31d39bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer set up for classification finetuning (training not executed)\n",
      "Classification evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 3: CLASSIFICATION WITH CHAT TEMPLATES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def prepare_classification_dataset():\n",
    "    # Load a small subset of a sentiment classification dataset\n",
    "    dataset = load_dataset(\"imdb\", split=\"train[:2000]\")\n",
    "\n",
    "    # Create classification prompts using chat template\n",
    "    formatted_data = []\n",
    "    for item in dataset:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies movie reviews as positive or negative.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Classify the following movie review as positive or negative:\\n\\n{item['text']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"positive\" if item[\"label\"] == 1 else \"negative\"}\n",
    "        ]\n",
    "        formatted_data.append({\"messages\": messages})\n",
    "\n",
    "    # Convert to dataset\n",
    "    classification_dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "    # Split into train and validation\n",
    "    dataset_split = classification_dataset.train_test_split(\n",
    "        test_size=config.val_set_size, seed=SEED\n",
    "    )\n",
    "\n",
    "    return dataset_split\n",
    "\n",
    "classification_dataset = prepare_classification_dataset()\n",
    "print(f\"Classification dataset size: {len(classification_dataset['train'])} training, {len(classification_dataset['test'])} validation examples\")\n",
    "\n",
    "# Show an example\n",
    "print(\"\\nExample classification from dataset:\")\n",
    "example_idx = 0\n",
    "example_class = classification_dataset[\"train\"][example_idx][\"messages\"]\n",
    "for message in example_class:\n",
    "    if message['role'] == 'user':\n",
    "        print(f\"{message['role'].upper()}: {message['content'][:100]}...\")\n",
    "    else:\n",
    "        print(f\"{message['role'].upper()}: {message['content']}\")\n",
    "\n",
    "# Tokenize classification data\n",
    "tokenized_classification_dataset = classification_dataset.map(\n",
    "    tokenize_chat_data,\n",
    "    batched=True,\n",
    "    remove_columns=classification_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# Classification training arguments\n",
    "classification_training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(config.output_dir, \"classification\"),\n",
    "    per_device_train_batch_size=config.micro_batch_size,\n",
    "    gradient_accumulation_steps=config.batch_size // config.micro_batch_size,\n",
    "    per_device_eval_batch_size=config.micro_batch_size,\n",
    "    learning_rate=config.learning_rate,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    # Replace 'evaluation_strategy' with 'eval_strategy'\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Setup Classification Trainer\n",
    "classification_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=classification_training_args,\n",
    "    train_dataset=tokenized_classification_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_classification_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer set up for classification finetuning (training not executed)\")\n",
    "\n",
    "# Evaluation function for classification\n",
    "def evaluate_classification(model, tokenizer, dataset, num_samples=100):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "\n",
    "    # Sample a subset for evaluation\n",
    "    sample_indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    samples = [dataset[i] for i in sample_indices]\n",
    "\n",
    "    for sample in tqdm(samples):\n",
    "        messages = sample[\"messages\"]\n",
    "        # Extract ground truth (assistant's message)\n",
    "        ground_truth_label = messages[-1][\"content\"].strip().lower()\n",
    "        ground_truth.append(1 if ground_truth_label == \"positive\" else 0)\n",
    "\n",
    "        # Create prompt without the answer\n",
    "        prompt_messages = messages[:-1]\n",
    "        prompt = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate classification\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.1,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        # Decode and extract prediction\n",
    "        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        prediction = \"positive\" if \"positive\" in output_text.lower() else \"negative\"\n",
    "        predictions.append(1 if prediction == \"positive\" else 0)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(ground_truth, predictions)\n",
    "    f1 = f1_score(ground_truth, predictions)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(ground_truth, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"Negative\", \"Positive\"],\n",
    "                yticklabels=[\"Negative\", \"Positive\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix (Accuracy: {accuracy:.3f}, F1: {f1:.3f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.output_dir, \"classification_confusion_matrix.png\"))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": cm\n",
    "    }\n",
    "\n",
    "print(\"Classification evaluation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "97Pgan0wrVkS"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wqEI0HExTPd"
   },
   "source": [
    "SECTION 4: EXTENDING CONTEXT LENGTH FOR TINYLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7HclUqu7xTwC",
    "outputId": "d8fa4eba-71ea-4d5d-9ea8-4905c49c6324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SECTION 4: EXTENDING CONTEXT LENGTH FOR TINYLLAMA\n",
      "==================================================\n",
      "Extended context model prepared. Maximum context length: 2048\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 4: EXTENDING CONTEXT LENGTH FOR TINYLLAMA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def prepare_extended_context_model():\n",
    "    \"\"\"\n",
    "    Configure a model with extended context length beyond TinyLlama's native capability\n",
    "    \"\"\"\n",
    "    # Create a new tokenizer with extended context size\n",
    "    extended_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.model_name,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "        model_max_length=config.extended_max_seq_length\n",
    "    )\n",
    "\n",
    "    if extended_tokenizer.pad_token is None:\n",
    "        extended_tokenizer.pad_token = extended_tokenizer.eos_token\n",
    "        extended_tokenizer.pad_token_id = extended_tokenizer.eos_token_id\n",
    "\n",
    "    # Configure model for extended context length\n",
    "    extended_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Apply positional embeddings extension\n",
    "    orig_max_positions = extended_model.config.max_position_embeddings\n",
    "    target_max_positions = config.extended_max_seq_length\n",
    "\n",
    "    # Check if extension is needed\n",
    "    if target_max_positions > orig_max_positions:\n",
    "        print(f\"Extending positional embeddings from {orig_max_positions} to {target_max_positions}\")\n",
    "\n",
    "        # Resize position embeddings to extended context length\n",
    "        extended_model.resize_token_embeddings(len(extended_tokenizer))\n",
    "\n",
    "        # Extend position embeddings (only needed for models with absolute positional embeddings)\n",
    "        if hasattr(extended_model, \"get_input_embeddings\"):\n",
    "            # Get current position embeddings\n",
    "            current_max_pos, embed_size = extended_model.get_input_embeddings().position_embeddings.weight.shape\n",
    "\n",
    "            # Create new position embeddings\n",
    "            new_embeddings = extended_model.get_input_embeddings().position_embeddings.weight.new_empty(\n",
    "                target_max_positions, embed_size\n",
    "            )\n",
    "\n",
    "            # Copy the old embeddings\n",
    "            k = 2.0\n",
    "            if current_max_pos < target_max_positions:\n",
    "                # Copy embeddings and extend them\n",
    "                new_embeddings[:current_max_pos] = extended_model.get_input_embeddings().position_embeddings.weight\n",
    "\n",
    "                # Initialize the rest with extrapolation\n",
    "                for i in range(current_max_pos, target_max_positions):\n",
    "                    if i > k * current_max_pos:\n",
    "                        new_embeddings[i] = new_embeddings[int(i / k)]\n",
    "                    else:\n",
    "                        new_embeddings[i] = new_embeddings[current_max_pos - 1]\n",
    "\n",
    "            # Update position embeddings\n",
    "            extended_model.get_input_embeddings().position_embeddings.weight.data = new_embeddings\n",
    "            extended_model.config.max_position_embeddings = target_max_positions\n",
    "\n",
    "    # Prepare for LoRA training\n",
    "    extended_model = prepare_model_for_kbit_training(extended_model)\n",
    "    extended_model = get_peft_model(extended_model, peft_config)\n",
    "\n",
    "    return extended_tokenizer, extended_model\n",
    "\n",
    "# Create a model and tokenizer with extended context length\n",
    "extended_tokenizer, extended_model = prepare_extended_context_model()\n",
    "print(f\"Extended context model prepared. Maximum context length: {extended_tokenizer.model_max_length}\")\n",
    "\n",
    "# Prepare a long context dataset\n",
    "def prepare_long_context_dataset():\n",
    "    # Create or load a dataset with long documents\n",
    "    # For demonstration, we'll generate synthetic long texts\n",
    "\n",
    "    # Load base dataset and concatenate samples to get long texts\n",
    "    base_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:500]\")\n",
    "\n",
    "    long_texts = []\n",
    "    current_text = \"\"\n",
    "    target_length = config.extended_max_seq_length * 0.9  # 90% of max length\n",
    "\n",
    "    for item in base_dataset:\n",
    "        current_text += item[\"text\"] + \"\\n\\n\"\n",
    "\n",
    "        # When we have enough text, create a sample\n",
    "        if len(current_text) >= target_length:\n",
    "            # Create a summarization task\n",
    "            text_chunk = current_text[:int(target_length)]\n",
    "\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes long texts.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Summarize the following text in 3-5 sentences:\\n\\n{text_chunk}\"},\n",
    "                {\"role\": \"assistant\", \"content\": \"Here is a summary of the text in 3-5 sentences: [This is a placeholder summary that would be generated during training]\"}\n",
    "            ]\n",
    "\n",
    "            long_texts.append({\"messages\": messages})\n",
    "            current_text = \"\"\n",
    "\n",
    "    # Convert to dataset\n",
    "    long_context_dataset = Dataset.from_list(long_texts)\n",
    "\n",
    "    # Split into train and validation\n",
    "    dataset_split = long_context_dataset.train_test_split(\n",
    "        test_size=config.val_set_size, seed=SEED\n",
    "    )\n",
    "\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "prdNofxOrXAD"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620,
     "referenced_widgets": [
      "8867ad32521749a7ad8645b4e52335d0",
      "6852751db3c14787aaa9f19790aec8f2",
      "80e29e3a916f4f099415c12158a111d8",
      "af2f267a9c7b481f8a7dc93336c85bea",
      "a64a3da71eb346b6abe156046f2a3a6f",
      "28a35735f7fd4dfcbc6d8f2a18709ec7",
      "aa05e750bb214270b498533894f9b18f",
      "b4dcd1136ede47f88c9d6b87cbabe0e0",
      "a699eca905fc40078e90d9cbf276cb54",
      "288e79f8e03a4f6b91c18133a452a779",
      "a786fb8f62b9446d93f648d46bd6602c",
      "fde41c779399448694adc2948b7864f6",
      "f4a54b96da234fff8ca95f3649d77ff3",
      "fc6a2183cde541ed95d52e18c4b5550e",
      "978dc328762449f5bedb02e367f01c21",
      "07fad44e3796448db16f7cc74cc4367b",
      "136495dc8c5843e3b4e963c13d5af572",
      "e2bd7b177fd742239f71bd7d59059499",
      "dcdc8e6c3e84488b9c39087e4e770321",
      "af2aee98b8ac4ffd862a596c48e1baf5",
      "8fe6000b76e24543b0ca633ce102d3a8",
      "3747e7bde9d74178999a618566695394",
      "4c7f1763713d48e59b29c51c1543cbf9",
      "e34db5b34dd14aa2aca55b4330f24e84",
      "3182dc507f1d458ea64f09037ae4e70a",
      "6cd7393121ee41d5bb2bab929b3bfcbd",
      "adc4394e22af45d986518693a69a23ed",
      "dbd28097644f43d59de4e68f0e4792e7",
      "1c45e25556534e369a8731634670ae10",
      "e8b6689cb3c042e796a7d66576f602d2",
      "12cd0bcefc184ad7b99902000b29c0c8",
      "3fb9ba96cde548b2a0654cc0d8ea963f",
      "f476d112bc494675956dac9b1b024b1e",
      "591b9f62dde84c00bb40bb93e3f2e9db",
      "1da59763d203407ab9289a7626db60e7",
      "950d8d375bdd452ba942d982e2885b99",
      "16c3452dd93249d4974ab6e5a04387d7",
      "d61d5ec789024e66b6894ce136b4c807",
      "3343bea31f7541f685ab3034302e0ee1",
      "93a4c4e16be648feb1e3facb4ff4bfb1",
      "dc750a244e8e4126bcf1f0a5dc45883d",
      "d3308617cfce4c6c88a6a0f5b4798930",
      "7d3283bf01fd4612a177ee807fb2b74e",
      "a96d9bfd9bc64a98a0b301cc9a41741d",
      "dcb635ef44644d45a7eda11cb0e750cb",
      "a45a6ad737c34a289a074abe4857851c",
      "407f11174d154ef68d662544addebfcc",
      "9d25766cea0c4d05a278faf6d6fed1d8",
      "17efc98bbfb74143baec954b85a9aade",
      "579d9415683e4ec8bbc3339f1f4a3243",
      "755870b728ce4d78942c1c2e5b92fa7f",
      "b4fc63d47bd94129ac270f1aa6bd1d2a",
      "6479583dea3741ffa123b2740c66d99a",
      "75fa3fa198dc4fdc8dd56f67b26fcceb",
      "a8e533c0a83749019fbbac578b88f8e7",
      "8ee5aa870f27445eb8f3c89f20d7fdc5",
      "964428571f6a447ba7f70f179f36eec8",
      "25d41a0c579a498cbdf1690742c95c40",
      "af9861423dc14492bc29ad0dfc7caf12",
      "73819d0f3e64473ab675e025e16b04ed",
      "9ea2153c38324d2a9f0d45f488936589",
      "7a894f8aa75c4110bf33d8268521d7c5",
      "bc51df64f96a4f6ea67e95647ac0d047",
      "1b3955161c5e43ca9e7c7870a23aad17",
      "05ea8bb95f184058be54e83563ac7b3d",
      "277a011c66e746b4a75a52cbb5d1885f",
      "115cfb5cfece408cbe7cac148fda1992",
      "5e27a32d49b845279da51e5313cc1cb9",
      "a1ca3b24149649b4ac31b46efe4c2bbd",
      "63c2fc1ba03c42de9c4fdb47606788f1",
      "ba15b90f471a4eaca597e5e100afcd03",
      "82a1ba5bd5ba495ebca4528934351098",
      "87b1873d0add449e92d8a03af38e4cf0",
      "649d8603898140408bd568bd39a07cb3",
      "f4ce26047cc34a80abcc0f7c8cdda947",
      "bc47d5a340e34618a6097bdcb9eba64a",
      "b41353c890614118a1663bdac1701436",
      "4c0e9452344c42028059f20d0720f058",
      "e911518ff17949bc869b19aeb10baf32",
      "cb5a78d9374b47e9ac9dbaf14093e675",
      "5b1af498f0334775b0709fb3d7edec39",
      "660354dbc60a45e4acd99161897c10bb",
      "50de30489d794b32823465327c2e52aa",
      "f4f6267c37884a259e7a40f894c2b76f",
      "cc1a9928807b4b0d96d69443c1f1c9ac",
      "06816242409f4b91ab2967613a6da653",
      "d290168e0cb94526b41e25aa237fd9db",
      "c488c0cd325b4c55b497529b6ad7abb6",
      "2f3011bbf1304578b875d9d3cda3f18e",
      "c687298aa9c940a391b290d9bba663d5",
      "383a14c5913b4398b95d1548e98691cd",
      "9295046ef7744c09bcb4cdf809d8f0ff",
      "ed9ff1a78331433fb8544658726ba79b",
      "6ee4dea1ad8944c391d67a52df8578d4",
      "f879707c24944221b4d7ad649f768b67",
      "33d7ce3548664a5282db7bd83e8e4d6a",
      "f6661b75b51946528d8f8c8fe443b678",
      "0933c8bcc49f4c54b9d3cefaf3ae4d40",
      "15fe869653dc4bb0957aaee8aff31ecc"
     ]
    },
    "id": "bp3qm_Dtxc7q",
    "outputId": "e197be89-2a63-4b61-cb7a-781fbd08b2ab"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8867ad32521749a7ad8645b4e52335d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde41c779399448694adc2948b7864f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7f1763713d48e59b29c51c1543cbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591b9f62dde84c00bb40bb93e3f2e9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb635ef44644d45a7eda11cb0e750cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee5aa870f27445eb8f3c89f20d7fdc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115cfb5cfece408cbe7cac148fda1992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long context dataset size: 46 training, 6 validation examples\n",
      "\n",
      "Example long context from dataset:\n",
      "SYSTEM: You are a helpful assistant that summarizes long texts.\n",
      "USER: Summarize the following text in 3-5 sentences:\n",
      "\n",
      "\n",
      "\n",
      " = = = Release = = = \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " In September 2010 , a t... [long content] ...s with a copy of Valkyria Chronicles III could download and apply the patch , which translated the g\n",
      "ASSISTANT: Here is a summary of the text in 3-5 sentences: [This is a placeholder summary that would be generated during training]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0e9452344c42028059f20d0720f058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3011bbf1304578b875d9d3cda3f18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer set up for long context finetuning (training not executed)\n"
     ]
    }
   ],
   "source": [
    "long_context_dataset = prepare_long_context_dataset()\n",
    "print(f\"Long context dataset size: {len(long_context_dataset['train'])} training, {len(long_context_dataset['test'])} validation examples\")\n",
    "\n",
    "# Show an example\n",
    "print(\"\\nExample long context from dataset:\")\n",
    "example_idx = 0\n",
    "example_long = long_context_dataset[\"train\"][example_idx][\"messages\"]\n",
    "for message in example_long:\n",
    "    role = message[\"role\"].upper()\n",
    "    content = message[\"content\"]\n",
    "    if role == \"USER\":\n",
    "        # Show just beginning and end of long content\n",
    "        if len(content) > 200:\n",
    "            print(f\"{role}: {content[:100]}... [long content] ...{content[-100:]}\")\n",
    "        else:\n",
    "            print(f\"{role}: {content}\")\n",
    "    else:\n",
    "        print(f\"{role}: {content}\")\n",
    "\n",
    "# Tokenize long context data\n",
    "def tokenize_long_context_data(examples):\n",
    "    formatted_chats = [\n",
    "        extended_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        for messages in examples[\"messages\"]\n",
    "    ]\n",
    "\n",
    "    return extended_tokenizer(\n",
    "        formatted_chats,\n",
    "        truncation=True,\n",
    "        max_length=config.extended_max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "# Apply tokenization to long context dataset\n",
    "tokenized_long_context_dataset = long_context_dataset.map(\n",
    "    tokenize_long_context_data,\n",
    "    batched=True,\n",
    "    remove_columns=long_context_dataset[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# Long context training arguments\n",
    "long_context_training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(config.output_dir, \"long_context\"),\n",
    "    per_device_train_batch_size=1,  # Reduced batch size for long sequences\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=config.learning_rate,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",\n",
    "    gradient_checkpointing=True,  # Enable to save memory\n",
    ")\n",
    "\n",
    "# Initialize data collator for long context\n",
    "long_context_data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=extended_tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Setup long context trainer\n",
    "long_context_trainer = Trainer(\n",
    "    model=extended_model,\n",
    "    args=long_context_training_args,\n",
    "    train_dataset=tokenized_long_context_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_long_context_dataset[\"test\"],\n",
    "    data_collator=long_context_data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer set up for long context finetuning (training not executed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aelkaUE8rYWT"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNY7_aFYxfP8"
   },
   "source": [
    "SECTION 5: MULTI-DATASET FINETUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423,
     "referenced_widgets": [
      "f00b317a5edb4cd39e20482b7914a6dd",
      "ee9c2bf0605741fb95a26f6b53b540a3",
      "e2fa1fcb9bc44e5db32b072a95d24c93",
      "d2071891e73c44edae2da0e0efc401c9",
      "a35904173cb64fb6a3044d9ad620cc57",
      "c8aac2fc41ea445cae2110784e029b0c",
      "2451e19ef1d84e108654ebc745ddcf5e",
      "1984aa0cab5445fc9a314aae02555344",
      "875872afc51546b0b457e9204200848c",
      "99c07beeeaab48a2a31fa3f59c7b686f",
      "13c606677ad74e6bbcd90e103c62f42c",
      "c2d5c5637c92496fa5dac2e5b12827b1",
      "17c05aefae474241b5f95f7fbee3ed4e",
      "7115fefafbc7429bbc99537a95c84e5d",
      "7fcdb495a4824affb6b292d0d48af46b",
      "76021158b33d4bca95b0472ff8a88e78",
      "e8704b6b9e2d4bb198b8068e1241d2c9",
      "c980e061b6924d389783c6dff567b6c1",
      "ed01019eaef6459d9bde4c1dcb0b7af0",
      "c8c3cb534efd4e2a836131de05a8e644",
      "1cab65f4d50143729bad394069666d69",
      "5c6fdbe161404eccb070cf52196c3dd5",
      "41f41dead9124c0db49dbc98abfe825a",
      "da90f9e75d34406ba1be9f1fed2901ad",
      "eff64eb7d3c44300a357d807a726ae31",
      "4e458a4b226848c88d44af136188b4cf",
      "fe8ffdbff5eb48b3a889702d071f9521",
      "3c0d430cab774560b571aefd9e2777cc",
      "af4eb0e1e773430893d576500d058860",
      "38aa5bf153484650b106a8b547dfb51e",
      "2b1044f44e8347329ce1eb87938203e1",
      "33eb5a765b0340df94dc2cf4497d9581",
      "2c1e0602fc084da9acd4d86a4664b0d6",
      "9fee49846d0e42aa8b0af923aae843fa",
      "be0ef10f41814daba8322fed42077c87",
      "c4eaf356cb664079a94978980602d78e",
      "cb3bb80c38a8448fbd1c94e4d7c72e08",
      "96f284498ca5406ca4066db557543c35",
      "323f12ae0fc743d9bde70ecb7e60bc13",
      "880bb15a0dea4798b5455b2825ec9053",
      "e0bffd620118486f813e5c8dc5e75185",
      "2fa047d2a5cf4bf9b807704b4dc8e1fd",
      "368b1a6b9ecf4eb9924c4be1e8aad961",
      "bfe617532654468e8438e21d6c3cf770",
      "34092f1e26aa44988b5682701571c3c3",
      "e211b181836947b7b68746c224b3e040",
      "9e3f73dbd6e14c67946b013ac2d967d3",
      "9c6af64bbb074ee5bc8bd4015fd4cad6",
      "0fee795cbe864a739c7bd669cc283e64",
      "2752f27cdabd4a34940bf56481ec4419",
      "76ccaf58b3d442d689bf8ec18eeb52e3",
      "b3e2cece31914eca8ff0a2197dbe252b",
      "de93f17bd668451185b2320231ed60d8",
      "f0169ae6838c4790b8ddcf885d5e23ed",
      "50d89273a0b848f8a2e9a30175df1195"
     ]
    },
    "id": "GUrUx_KoxqOd",
    "outputId": "9845d33f-6480-4a76-a3ee-8c31d6f659fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SECTION 5: MULTI-DATASET FINETUNING\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00b317a5edb4cd39e20482b7914a6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d5c5637c92496fa5dac2e5b12827b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f41dead9124c0db49dbc98abfe825a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined multi-dataset size: 270 training, 30 validation examples\n",
      "\n",
      "Dataset distribution:\n",
      "  code: 90 examples (33.3%)\n",
      "  qa: 90 examples (33.3%)\n",
      "  translation: 90 examples (33.3%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fee49846d0e42aa8b0af923aae843fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34092f1e26aa44988b5682701571c3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer set up for multi-dataset finetuning (training not executed)\n",
      "Multi-dataset evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 5: MULTI-DATASET FINETUNING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def prepare_multiple_datasets():\n",
    "    \"\"\"\n",
    "    Prepare multiple datasets with different formats for a single training run\n",
    "    \"\"\"\n",
    "    # 1. Create a simple QA dataset\n",
    "    qa_data = []\n",
    "    for i in range(100):\n",
    "        num1 = np.random.randint(1, 100)\n",
    "        num2 = np.random.randint(1, 100)\n",
    "        result = num1 + num2\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful math assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"What is {num1} + {num2}?\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"The sum of {num1} and {num2} is {result}.\"}\n",
    "        ]\n",
    "        qa_data.append({\"messages\": messages, \"dataset_type\": \"qa\"})\n",
    "\n",
    "    # 2. Create a translation dataset\n",
    "    translations = [\n",
    "        (\"Hello, how are you?\", \"Hola, ¿cómo estás?\"),\n",
    "        (\"Good morning\", \"Buenos días\"),\n",
    "        (\"What's your name?\", \"¿Cómo te llamas?\"),\n",
    "        (\"I love programming\", \"Me encanta programar\"),\n",
    "        (\"This is a test\", \"Esto es una prueba\")\n",
    "    ]\n",
    "\n",
    "    translation_data = []\n",
    "    for i in range(100):\n",
    "        idx = i % len(translations)\n",
    "        english, spanish = translations[idx]\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful English to Spanish translator.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Translate this English text to Spanish: '{english}'\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"'{english}' translated to Spanish is: '{spanish}'\"}\n",
    "        ]\n",
    "        translation_data.append({\"messages\": messages, \"dataset_type\": \"translation\"})\n",
    "\n",
    "    # 3. Create code generation dataset\n",
    "    code_snippets = [\n",
    "        (\n",
    "            \"Write a Python function to calculate the factorial of a number.\",\n",
    "            \"\"\"def factorial(n):\n",
    "    if n == 0 or n == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)\"\"\"\n",
    "        ),\n",
    "        (\n",
    "            \"Create a JavaScript function that checks if a string is a palindrome.\",\n",
    "            \"\"\"function isPalindrome(str) {\n",
    "    const cleanStr = str.toLowerCase().replace(/[^a-z0-9]/g, '');\n",
    "    return cleanStr === cleanStr.split('').reverse().join('');\n",
    "}\"\"\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    code_data = []\n",
    "    for i in range(100):\n",
    "        idx = i % len(code_snippets)\n",
    "        prompt, code = code_snippets[idx]\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": f\"Here's the solution:\\n\\n```\\n{code}\\n```\"}\n",
    "        ]\n",
    "        code_data.append({\"messages\": messages, \"dataset_type\": \"code\"})\n",
    "\n",
    "    # Create datasets for each type\n",
    "    qa_dataset = Dataset.from_list(qa_data)\n",
    "    translation_dataset = Dataset.from_list(translation_data)\n",
    "    code_dataset = Dataset.from_list(code_data)\n",
    "\n",
    "    # Split each dataset\n",
    "    qa_split = qa_dataset.train_test_split(test_size=config.val_set_size, seed=SEED)\n",
    "    translation_split = translation_dataset.train_test_split(test_size=config.val_set_size, seed=SEED)\n",
    "    code_split = code_dataset.train_test_split(test_size=config.val_set_size, seed=SEED)\n",
    "\n",
    "    # Combine training sets with weights\n",
    "    # Add sampling weights as a column before combining\n",
    "    def add_weights(dataset, weight):\n",
    "        return dataset.map(lambda x: {\"weight\": weight})\n",
    "\n",
    "    qa_train_weighted = add_weights(qa_split[\"train\"], 1.0)\n",
    "    translation_train_weighted = add_weights(translation_split[\"train\"], 0.7)\n",
    "    code_train_weighted = add_weights(code_split[\"train\"], 1.2)  # Prioritize code examples\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_train = concatenate_datasets([\n",
    "        qa_train_weighted,\n",
    "        translation_train_weighted,\n",
    "        code_train_weighted\n",
    "    ])\n",
    "\n",
    "    # Combine validation sets\n",
    "    combined_val = concatenate_datasets([\n",
    "        qa_split[\"test\"],\n",
    "        translation_split[\"test\"],\n",
    "        code_split[\"test\"]\n",
    "    ])\n",
    "\n",
    "    # Use DatasetDict for compatibility with Trainer\n",
    "    combined_dataset = DatasetDict({\n",
    "        \"train\": combined_train,\n",
    "        \"validation\": combined_val\n",
    "    })\n",
    "\n",
    "    return combined_dataset, {\"qa\": qa_split, \"translation\": translation_split, \"code\": code_split}\n",
    "\n",
    "multi_dataset, individual_datasets = prepare_multiple_datasets()\n",
    "print(f\"Combined multi-dataset size: {len(multi_dataset['train'])} training, {len(multi_dataset['validation'])} validation examples\")\n",
    "\n",
    "# Show distribution of dataset types\n",
    "dataset_types = multi_dataset[\"train\"][\"dataset_type\"]\n",
    "unique_types, counts = np.unique(dataset_types, return_counts=True)\n",
    "print(\"\\nDataset distribution:\")\n",
    "for dtype, count in zip(unique_types, counts):\n",
    "    print(f\"  {dtype}: {count} examples ({count/len(dataset_types)*100:.1f}%)\")\n",
    "\n",
    "# Tokenize multi-dataset\n",
    "def tokenize_multi_dataset(examples):\n",
    "    formatted_chats = [\n",
    "        tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        for messages in examples[\"messages\"]\n",
    "    ]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        formatted_chats,\n",
    "        truncation=True,\n",
    "        max_length=config.max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Preserve metadata columns\n",
    "    if \"dataset_type\" in examples:\n",
    "        tokenized[\"dataset_type\"] = examples[\"dataset_type\"]\n",
    "    if \"weight\" in examples:\n",
    "        tokenized[\"weight\"] = examples[\"weight\"]\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization\n",
    "# Apply tokenization to multi-dataset\n",
    "tokenized_multi_dataset = multi_dataset.map(\n",
    "    tokenize_multi_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=[\"messages\"],  # Keep dataset_type and weight\n",
    ")\n",
    "\n",
    "# Create a weighted sampler for training data\n",
    "def create_weighted_sampler(dataset):\n",
    "    weights = torch.tensor(dataset[\"weight\"], dtype=torch.float)\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(\n",
    "        weights=weights,\n",
    "        num_samples=len(weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    return sampler\n",
    "\n",
    "# Multi-dataset training arguments\n",
    "multi_dataset_training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(config.output_dir, \"multi_dataset\"),\n",
    "    per_device_train_batch_size=config.micro_batch_size,\n",
    "    gradient_accumulation_steps=config.batch_size // config.micro_batch_size,\n",
    "    per_device_eval_batch_size=config.micro_batch_size,\n",
    "    learning_rate=config.learning_rate,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    # Replace 'evaluation_strategy' with 'eval_strategy'\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Initialize data collator\n",
    "multi_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Setup multi-dataset trainer with custom sampler\n",
    "class WeightedTrainer(Trainer):\n",
    "    def _get_train_sampler(self):\n",
    "        if \"weight\" in self.train_dataset.column_names:\n",
    "            return create_weighted_sampler(self.train_dataset)\n",
    "        return super()._get_train_sampler()\n",
    "\n",
    "multi_trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=multi_dataset_training_args,\n",
    "    train_dataset=tokenized_multi_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_multi_dataset[\"validation\"],\n",
    "    data_collator=multi_data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer set up for multi-dataset finetuning (training not executed)\")\n",
    "\n",
    "# Evaluation function for multi-dataset model\n",
    "def evaluate_multi_dataset(model, tokenizer, datasets, num_samples=10):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "\n",
    "    for dataset_name, dataset_split in datasets.items():\n",
    "        # Sample from test set\n",
    "        eval_dataset = dataset_split[\"test\"]\n",
    "        sample_indices = np.random.choice(len(eval_dataset), min(num_samples, len(eval_dataset)), replace=False)\n",
    "        samples = [eval_dataset[i] for i in sample_indices]\n",
    "\n",
    "        correct = 0\n",
    "        examples = []\n",
    "\n",
    "        for sample in samples:\n",
    "            messages = sample[\"messages\"]\n",
    "            # Get ground truth (last message)\n",
    "            ground_truth = messages[-1][\"content\"]\n",
    "\n",
    "            # Create prompt without the answer\n",
    "            prompt_messages = messages[:-1]\n",
    "            prompt = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "            # Tokenize input\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "\n",
    "            # Decode prediction\n",
    "            output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            prediction = output_text.split(prompt)[-1].strip()\n",
    "\n",
    "            # For simple evaluation - just check if any part of ground truth is in prediction\n",
    "            # In a real scenario, you'd use task-specific metrics\n",
    "            is_correct = any(segment in prediction.lower() for segment in ground_truth.lower().split())\n",
    "            correct += int(is_correct)\n",
    "\n",
    "            examples.append({\n",
    "                \"prompt\": prompt[-100:] + \"...\",  # Just show the end of the prompt\n",
    "                \"prediction\": prediction[:100] + (\"...\" if len(prediction) > 100 else \"\"),\n",
    "                \"ground_truth\": ground_truth[:100] + (\"...\" if len(ground_truth) > 100 else \"\"),\n",
    "                \"is_correct\": is_correct\n",
    "            })\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = correct / len(samples) if samples else 0\n",
    "\n",
    "        results[dataset_name] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"examples\": examples\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "print(\"Multi-dataset evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iCngc-gNraWw"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1zV5o0exq_M"
   },
   "source": [
    "SECTION 6: COMBINED TRAINING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LEUVZGR_xxEj",
    "outputId": "7b1a1da9-1a0a-4f44-a021-4a879b85c2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SECTION 6: COMBINED TRAINING PIPELINE\n",
      "==================================================\n",
      "LLM Finetuning with Chat Templates, Classification, Extended Context, and Multi-dataset Training\n",
      "================================================================================\n",
      "Base model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Default context size: 512\n",
      "Extended context size: 2048\n",
      "LoRA rank: 8\n",
      "LoRA alpha: 16\n",
      "Batch size: 4\n",
      "Learning rate: 0.0002\n",
      "Number of epochs: 3\n",
      "Starting full training pipeline...\n",
      "\n",
      "Step 1: Training on conversational data...\n",
      "\n",
      "Step 2: Training on classification data...\n",
      "\n",
      "Step 3: Training extended context model...\n",
      "\n",
      "Step 4: Training on multiple datasets...\n",
      "\n",
      "Training pipeline complete!\n",
      "\n",
      "Summary of Results:\n",
      "  - conversational: Training would execute here\n",
      "  - classification: Training would execute here\n",
      "  - extended_context: Training would execute here\n",
      "  - multi_dataset: Training would execute here\n",
      "\n",
      "Note: Actual training was not executed in this example.\n",
      "To run the actual training, uncomment the trainer.train() lines in the full_training_pipeline function.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 6: COMBINED TRAINING PIPELINE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def full_training_pipeline():\n",
    "    \"\"\"\n",
    "    Define a complete training pipeline that combines all approaches\n",
    "    \"\"\"\n",
    "    print(\"Starting full training pipeline...\")\n",
    "    results = {}\n",
    "\n",
    "    # Step 1: Train on conversational data\n",
    "    print(\"\\nStep 1: Training on conversational data...\")\n",
    "    # conv_trainer.train()\n",
    "    # model.save_pretrained(os.path.join(config.output_dir, \"model_conv\"))\n",
    "    # tokenizer.save_pretrained(os.path.join(config.output_dir, \"tokenizer_conv\"))\n",
    "    results[\"conversational\"] = \"Training would execute here\"\n",
    "\n",
    "    # Step 2: Train on classification data\n",
    "    print(\"\\nStep 2: Training on classification data...\")\n",
    "    # classification_trainer.train()\n",
    "    # model.save_pretrained(os.path.join(config.output_dir, \"model_classification\"))\n",
    "    # tokenizer.save_pretrained(os.path.join(config.output_dir, \"tokenizer_classification\"))\n",
    "\n",
    "    # Evaluate classification\n",
    "    # classification_metrics = evaluate_classification(model, tokenizer, classification_dataset[\"test\"])\n",
    "    # results[\"classification\"] = classification_metrics\n",
    "    results[\"classification\"] = \"Training would execute here\"\n",
    "\n",
    "    # Step 3: Train extended context model\n",
    "    print(\"\\nStep 3: Training extended context model...\")\n",
    "    # long_context_trainer.train()\n",
    "    # extended_model.save_pretrained(os.path.join(config.output_dir, \"model_extended_context\"))\n",
    "    # extended_tokenizer.save_pretrained(os.path.join(config.output_dir, \"tokenizer_extended_context\"))\n",
    "    results[\"extended_context\"] = \"Training would execute here\"\n",
    "\n",
    "    # Step 4: Train multi-dataset model\n",
    "    print(\"\\nStep 4: Training on multiple datasets...\")\n",
    "    # multi_trainer.train()\n",
    "    # model.save_pretrained(os.path.join(config.output_dir, \"model_multi_dataset\"))\n",
    "    # tokenizer.save_pretrained(os.path.join(config.output_dir, \"tokenizer_multi_dataset\"))\n",
    "\n",
    "    # Evaluate multi-dataset\n",
    "    # multi_dataset_metrics = evaluate_multi_dataset(model, tokenizer, individual_datasets)\n",
    "    # results[\"multi_dataset\"] = multi_dataset_metrics\n",
    "    results[\"multi_dataset\"] = \"Training would execute here\"\n",
    "\n",
    "    print(\"\\nTraining pipeline complete!\")\n",
    "    return results\n",
    "\n",
    "# Define main function that executes the training pipeline\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the full pipeline\n",
    "    \"\"\"\n",
    "    print(\"LLM Finetuning with Chat Templates, Classification, Extended Context, and Multi-dataset Training\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Display configurations\n",
    "    print(f\"Base model: {config.model_name}\")\n",
    "    print(f\"Default context size: {config.max_seq_length}\")\n",
    "    print(f\"Extended context size: {config.extended_max_seq_length}\")\n",
    "    print(f\"LoRA rank: {config.lora_r}\")\n",
    "    print(f\"LoRA alpha: {config.lora_alpha}\")\n",
    "    print(f\"Batch size: {config.batch_size}\")\n",
    "    print(f\"Learning rate: {config.learning_rate}\")\n",
    "    print(f\"Number of epochs: {config.num_epochs}\")\n",
    "\n",
    "    # Execute full pipeline\n",
    "    results = full_training_pipeline()\n",
    "\n",
    "    # Print summary of results\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    for dataset_type, result in results.items():\n",
    "        print(f\"  - {dataset_type}: {result}\")\n",
    "\n",
    "    print(\"\\nNote: Actual training was not executed in this example.\")\n",
    "    print(\"To run the actual training, uncomment the trainer.train() lines in the full_training_pipeline function.\")\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "COBenarhrbbw"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmnnWGaBxycg"
   },
   "source": [
    "SECTION 7: INFERENCE AND DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-hFs5-hx50t",
    "outputId": "a3b6ea2e-d7fc-41aa-d452-5d6c23ee2417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SECTION 7: INFERENCE AND DEPLOYMENT\n",
      "==================================================\n",
      "\n",
      "Inference examples:\n",
      "\n",
      "Model Inference Demos:\n",
      "Note: This is showing code structure only, not actual inference since training wasn't executed.\n",
      "\n",
      "----------------------------------------\n",
      "Model: Conversational\n",
      "Prompt: What's your favorite book and why?...\n",
      "Response would be generated here if model was trained\n",
      "\n",
      "----------------------------------------\n",
      "Model: Classification\n",
      "Prompt: Classify the following review: 'This movie was abs...\n",
      "Response would be generated here if model was trained\n",
      "\n",
      "----------------------------------------\n",
      "Model: Extended Context\n",
      "Prompt: Summarize the following text: This is a test of ex...\n",
      "Response would be generated here if model was trained\n",
      "\n",
      "----------------------------------------\n",
      "Model: Multi-Dataset\n",
      "Prompt: Write a Python function to find the nth Fibonacci ...\n",
      "Response would be generated here if model was trained\n",
      "\n",
      "Deployment Guide:\n",
      "1. Export your model for production:\n",
      "   ```python\n",
      "   # Merge LoRA weights with base model\n",
      "   merged_model = load_and_merge_model('path/to/finetuned/model')\n",
      "   # Save the merged model\n",
      "   merged_model.save_pretrained('path/to/export/model')\n",
      "   tokenizer.save_pretrained('path/to/export/tokenizer')\n",
      "   ```\n",
      "\n",
      "2. Quantize for deployment (optional):\n",
      "   ```python\n",
      "   from transformers import BitsAndBytesConfig\n",
      "   quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
      "   quantized_model = AutoModelForCausalLM.from_pretrained(\n",
      "       'path/to/export/model',\n",
      "       quantization_config=quantization_config,\n",
      "       device_map='auto'\n",
      "   )\n",
      "   ```\n",
      "\n",
      "3. Deploy using Hugging Face Inference Endpoints or other services\n",
      "   - Push to Hugging Face Hub:\n",
      "     ```python\n",
      "     from huggingface_hub import HfApi\n",
      "     api = HfApi()\n",
      "     api.create_repo('your-username/your-model-name')\n",
      "     merged_model.push_to_hub('your-username/your-model-name')\n",
      "     tokenizer.push_to_hub('your-username/your-model-name')\n",
      "     ```\n",
      "\n",
      "4. For local deployment, use FastAPI:\n",
      "   ```python\n",
      "   from fastapi import FastAPI\n",
      "   from pydantic import BaseModel\n",
      "   import torch\n",
      "   from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "   \n",
      "   app = FastAPI()\n",
      "   \n",
      "   # Load model and tokenizer\n",
      "   model = AutoModelForCausalLM.from_pretrained('path/to/export/model', device_map='auto')\n",
      "   tokenizer = AutoTokenizer.from_pretrained('path/to/export/tokenizer')\n",
      "   \n",
      "   class ChatRequest(BaseModel):\n",
      "       messages: list\n",
      "   \n",
      "   @app.post('/generate')\n",
      "   def generate_text(request: ChatRequest):\n",
      "       formatted_prompt = tokenizer.apply_chat_template(request.messages, tokenize=False, add_generation_prompt=True)\n",
      "       inputs = tokenizer(formatted_prompt, return_tensors='pt').to(model.device)\n",
      "       outputs = model.generate(**inputs, max_new_tokens=100)\n",
      "       response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
      "       return {'response': response.split(formatted_prompt)[-1]}\n",
      "   ```\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SECTION 7: INFERENCE AND DEPLOYMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def load_and_merge_model(model_path):\n",
    "    \"\"\"\n",
    "    Load a trained LoRA model and merge weights with base model\n",
    "    \"\"\"\n",
    "    # Load base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Load and merge LoRA weights\n",
    "    from peft import PeftModel\n",
    "\n",
    "    # Load the LoRA model\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        model_path,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Merge LoRA weights with base model\n",
    "    merged_model = model.merge_and_unload()\n",
    "\n",
    "    return merged_model\n",
    "\n",
    "def inference_demo(model, tokenizer, prompt, max_length=100):\n",
    "    \"\"\"\n",
    "    Run inference on a model with a given prompt\n",
    "    \"\"\"\n",
    "    # Format the prompt using chat template\n",
    "    if isinstance(prompt, list):  # If prompt is a list of message dicts\n",
    "        formatted_prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    else:  # If prompt is a string\n",
    "        formatted_prompt = prompt\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_length,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    # Decode and return\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def demo_all_models():\n",
    "    \"\"\"\n",
    "    Demonstrate inference with all trained models\n",
    "    \"\"\"\n",
    "    # Note: In a real scenario, you would load actual trained models from disk\n",
    "    # For this example, we'll just show the code structure\n",
    "\n",
    "    # List of model types and test prompts\n",
    "    model_demos = [\n",
    "        {\n",
    "            \"name\": \"Conversational\",\n",
    "            \"path\": os.path.join(config.output_dir, \"model_conv\"),\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"What's your favorite book and why?\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Classification\",\n",
    "            \"path\": os.path.join(config.output_dir, \"model_classification\"),\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies movie reviews as positive or negative.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Classify the following review: 'This movie was absolutely fantastic! I loved the plot and the acting was superb.'\"}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Extended Context\",\n",
    "            \"path\": os.path.join(config.output_dir, \"model_extended_context\"),\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes long texts.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Summarize the following text: \" + \"This is a test of extended context. \" * 100}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Multi-Dataset\",\n",
    "            \"path\": os.path.join(config.output_dir, \"model_multi_dataset\"),\n",
    "            \"prompt\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Write a Python function to find the nth Fibonacci number.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"\\nModel Inference Demos:\")\n",
    "    print(\"Note: This is showing code structure only, not actual inference since training wasn't executed.\")\n",
    "\n",
    "    for demo in model_demos:\n",
    "        print(f\"\\n{'-'*40}\")\n",
    "        print(f\"Model: {demo['name']}\")\n",
    "        print(f\"Prompt: {demo['prompt'][-1]['content'][:50]}...\")\n",
    "        print(f\"Response would be generated here if model was trained\")\n",
    "        # In a real scenario:\n",
    "        # model = load_and_merge_model(demo[\"path\"])\n",
    "        # response = inference_demo(model, tokenizer, demo[\"prompt\"])\n",
    "        # print(f\"Response: {response}\")\n",
    "\n",
    "# Deployment instructions\n",
    "def print_deployment_guide():\n",
    "    \"\"\"\n",
    "    Print instructions for deploying the finetuned models\n",
    "    \"\"\"\n",
    "    print(\"\\nDeployment Guide:\")\n",
    "    print(\"1. Export your model for production:\")\n",
    "    print(\"   ```python\")\n",
    "    print(\"   # Merge LoRA weights with base model\")\n",
    "    print(\"   merged_model = load_and_merge_model('path/to/finetuned/model')\")\n",
    "    print(\"   # Save the merged model\")\n",
    "    print(\"   merged_model.save_pretrained('path/to/export/model')\")\n",
    "    print(\"   tokenizer.save_pretrained('path/to/export/tokenizer')\")\n",
    "    print(\"   ```\")\n",
    "\n",
    "    print(\"\\n2. Quantize for deployment (optional):\")\n",
    "    print(\"   ```python\")\n",
    "    print(\"   from transformers import BitsAndBytesConfig\")\n",
    "    print(\"   quantization_config = BitsAndBytesConfig(load_in_4bit=True)\")\n",
    "    print(\"   quantized_model = AutoModelForCausalLM.from_pretrained(\")\n",
    "    print(\"       'path/to/export/model',\")\n",
    "    print(\"       quantization_config=quantization_config,\")\n",
    "    print(\"       device_map='auto'\")\n",
    "    print(\"   )\")\n",
    "    print(\"   ```\")\n",
    "\n",
    "    print(\"\\n3. Deploy using Hugging Face Inference Endpoints or other services\")\n",
    "    print(\"   - Push to Hugging Face Hub:\")\n",
    "    print(\"     ```python\")\n",
    "    print(\"     from huggingface_hub import HfApi\")\n",
    "    print(\"     api = HfApi()\")\n",
    "    print(\"     api.create_repo('your-username/your-model-name')\")\n",
    "    print(\"     merged_model.push_to_hub('your-username/your-model-name')\")\n",
    "    print(\"     tokenizer.push_to_hub('your-username/your-model-name')\")\n",
    "    print(\"     ```\")\n",
    "\n",
    "    print(\"\\n4. For local deployment, use FastAPI:\")\n",
    "    print(\"   ```python\")\n",
    "    print(\"   from fastapi import FastAPI\")\n",
    "    print(\"   from pydantic import BaseModel\")\n",
    "    print(\"   import torch\")\n",
    "    print(\"   from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
    "    print(\"   \")\n",
    "    print(\"   app = FastAPI()\")\n",
    "    print(\"   \")\n",
    "    print(\"   # Load model and tokenizer\")\n",
    "    print(\"   model = AutoModelForCausalLM.from_pretrained('path/to/export/model', device_map='auto')\")\n",
    "    print(\"   tokenizer = AutoTokenizer.from_pretrained('path/to/export/tokenizer')\")\n",
    "    print(\"   \")\n",
    "    print(\"   class ChatRequest(BaseModel):\")\n",
    "    print(\"       messages: list\")\n",
    "    print(\"   \")\n",
    "    print(\"   @app.post('/generate')\")\n",
    "    print(\"   def generate_text(request: ChatRequest):\")\n",
    "    print(\"       formatted_prompt = tokenizer.apply_chat_template(request.messages, tokenize=False, add_generation_prompt=True)\")\n",
    "    print(\"       inputs = tokenizer(formatted_prompt, return_tensors='pt').to(model.device)\")\n",
    "    print(\"       outputs = model.generate(**inputs, max_new_tokens=100)\")\n",
    "    print(\"       response = tokenizer.decode(outputs[0], skip_special_tokens=True)\")\n",
    "    print(\"       return {'response': response.split(formatted_prompt)[-1]}\")\n",
    "    print(\"   ```\")\n",
    "\n",
    "# Call demo functions\n",
    "print(\"\\nInference examples:\")\n",
    "demo_all_models()\n",
    "print_deployment_guide()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "m92Re4XVrdgP"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "VBS3GcLVpgTq"
   },
   "outputs": [],
   "source": [
    "!nbstripout Phi-3.5ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
