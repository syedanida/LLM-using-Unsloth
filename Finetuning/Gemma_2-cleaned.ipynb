{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LbgzyNZO9Jta",
    "outputId": "3298eacc-656b-49fc-e53a-c08ee38345cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.7/192.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.8/127.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
      "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow-metadata 1.17.0 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install -q unsloth datasets transformers accelerate bitsandbytes peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iStaBPpB9VCt"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from datasets import load_dataset\n",
    "#Import SFTTrainer from unsloth.trainer\n",
    "from unsloth.trainer import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "c9d29ca5cb534e5cb556af8b0081555c",
      "c2026cc0a8e64fb7ae7b9400220b8178",
      "6a6815362c2a467d90860da2a4cd35b3",
      "30503dbae3044b95b78b155e2d38bd9f",
      "0d938665d8334107bd0ab25ea3aa53c9",
      "e3461f57c6494b86a395ada67d087474",
      "ef96c29118144d4b8d381edf7a979d5e",
      "75f1dcc246b9477dabd8175835afd5e9",
      "c065cec0b2c84b50ab2b1c67459ab975",
      "1b0fdb63fef6481bb155ec3377991578",
      "908ce58708394cfe958a6f3c5ffd0211",
      "50b9cfc9c7f347c49154af1cdf993b24",
      "4212cf37cbf148f7aa07b526331a1c40",
      "1ee21f4771ad4388943246deb57aae18",
      "9e0f909617a145adab4e8d5de160a770",
      "57e79a8e82b54d77883c0a2aeba9a0e5",
      "90c629c2b87b48ff80892e5d191f4a3a",
      "71205ea2429e428693d521bc4b877d91",
      "facd6c98663a47e5aafac6296bec784e",
      "74d6bd976ed04b3e99df1da914f19460",
      "1b9a947814b140dda4b4ae993d64f2b2",
      "c68b4d9cf1ee4d9ab6fe1450389d0c8d",
      "9f47da8c6d2e46aea318da17767548d7",
      "fa840646c8df48e8b2f4ff4ad3068cc9",
      "18777fe364054489964506bb85578258",
      "f2d96600b3ca4704b659555a747e60e4",
      "2ab29ec5907e4a01bdce5a4b65b3e8f5",
      "f1ee17ac4ea540f1b4b050d98eedb1ba",
      "7b67c0a8cdd54aea975f363e299ec57a",
      "56d0fa37a03a4a5185990e6ab64f412b",
      "a492afdc8caf4026a196fae03a973e01",
      "4177d3da555b4e56ad0f3fe623d231e3",
      "c4bf0a6da08f40bb8670dcfca392c362",
      "eb4f26977bf6439797cadb1e323ab24d",
      "575eb83735ad499a82360237b5eb3140",
      "0f216ef360194ff3bda5c44104919dbb",
      "c89204fa205448cc9185c9204fc41f8c",
      "0d32b4dda963443e8f59122279b0437a",
      "b155fb4072b5436d93bc204aefec1949",
      "6baac999f987451e807d72fef72bb934",
      "8c98a0742be74da6bd16155759cb7e57",
      "5da266154c484f09853bbc884c2ebe38",
      "c4e6fc497f9a4dbb94376be4b0d6a549",
      "3aa45b321e29464793ef6dbccce1432e",
      "8927a8c29f5e4fd2b2bc47a475d16bd5",
      "39e7f1a900c74f7193da38e1514a199a",
      "34f46cd83e6c4f9194cdccfafa13b815",
      "0de85efe5ca14ed4b377023947dbd30e",
      "4412c808e9c44658b4a03635c89dee2b",
      "c9d19c6125f64ae399ea2b255880131c",
      "d8233c10e7504631af3f1245f9a0c055",
      "143e03171d14451ea1d3625cbbf2cc06",
      "5ff9605789104058b3932bb1460e6f74",
      "a4b755c1e9f64e2abcb724f9ac115bc9",
      "9249857d3ed0481c98c2a23d52e48276",
      "6665bfb6bdeb434cb4c581cfd5f7039e",
      "a4e826c7ee644e7581c9b6ab89993e7f",
      "8fba414641a046d9a3e8dc902c578b9a",
      "99ea9e747d6b4669bbbd0f2b4851312a",
      "fcd3c4317e8a4209862e73b3f2d706cf",
      "efa148963f944d7dac60ed1291deb181",
      "4845cad597f047d9b5aa6ab7eb2546b6",
      "cf579b3fa0284046a233ef5d0815f9b3",
      "2203a3b9450e4716a60dd334a05101e4",
      "d26121f95f564d7faf579bbd2e68bd26",
      "8806ee43b5014bd9add848917fa74d37"
     ]
    },
    "id": "miYHtM5Z9Wz8",
    "outputId": "fea0d1b6-cc3d-4d16-cb35-95f2981944a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Gemma patching. Transformers: 4.50.3.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d29ca5cb534e5cb556af8b0081555c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b9cfc9c7f347c49154af1cdf993b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f47da8c6d2e46aea318da17767548d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4f26977bf6439797cadb1e323ab24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8927a8c29f5e4fd2b2bc47a475d16bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6665bfb6bdeb434cb4c581cfd5f7039e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Gemma 9B with Unsloth\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-2b-it\",\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "14118b4463054fdab70515a3b83950d2",
      "6af0879c830045da934d949ba519d327",
      "06072d01a79c421fb1e1b5b8b1a5dc83",
      "bb2480f8b3a341298edbf42e0d2287fa",
      "49152c7b67a8426190b83955b1cd9f5b",
      "5bf77105764549c891bbe38a251a121e",
      "1753158f5760447fa9cd64c2d48e04f5",
      "9bac45bda4ce45b08f5021e5a87b92ce",
      "87b163ad955a44288cc97d8b32ef2b7c",
      "f54f2a3d830b43dc9861f36041f46aa2",
      "6bef1f98bf784f02a01d89a7bef6c207",
      "80a48039ecb149d18248baebb65013e1",
      "5973852b3a21487f8a5da2377b89e0d4",
      "34b6a64a211d4c988eea061ea6746c62",
      "b3080f4fb9244eeeadc7a9cebcf3065b",
      "1bf1419c16374fb8a2a1da203304f0a8",
      "8141abc504fb427384c9caffa4fa1dd6",
      "4e0a16ae335d4a46ad758041d27e8d6c",
      "b8d6151c558c49e6b1de252ee9c4612c",
      "312e3a9c90cc4217afd806c09db03ac2",
      "cbeb16eba30a4550a4f0e856b31783b7",
      "fee78b74ee0143da886418b48f2fb665",
      "20b9a71b42ae404082ed258fbad6bbf0",
      "d5ac0c31cbc94b1c8f012da2babf1cac",
      "5e58edb5acab4443a2e0917657594724",
      "906202024e2b4ddab3a0fe4bdc1dafc6",
      "9a89d661aaf14b1e8378c13438183b4c",
      "10156e171d134c21b3fa634d0e2eece6",
      "465522a39938494e958cbc898c2357ab",
      "a3f89c0656234bacb2f916cca236f8eb",
      "776729c8cad546bea5da4c26c45cb687",
      "a22a6b0752d6484cbd8c8a6147456ac1",
      "7fbda439899444c08e2bc1e7e0bd9be4",
      "ba569b9a1910445db923271d96112b4b",
      "c6f68ab2583041b6a16d0b38ab9ebcfd",
      "cd4684e896684ca4a52a0f8621741681",
      "a28dadccaebb471f8e0b804ba9302e75",
      "1d3e602763064e2b878c652309727a92",
      "71c9514910514f98845cab4876bd0aab",
      "0e6629b4783f4e2993e5690aa8d19bfc",
      "6b1947d969bd4f7aae8c9ea25e23d69d",
      "3ba7d18463484660b70d541a86db02ae",
      "dc7d6258fa92402f8575f1f78460820a",
      "a773fe649aee4279b237ac26728dac83",
      "2cd46ca356a64dcbaf727e0107fb939c",
      "fd677540a669433098e651b7913d50bf",
      "ad5917cb616847eb88f07fa2445a1c4c",
      "90916c21883148bb9f03867ac17b8dda",
      "501b4a04a85f4aa4965d6d3742e9ead6",
      "6536742bf13c4e2b8b055c00cdd3efd8",
      "50c910d73d5648f187671c7763d6a4cb",
      "3a013913ebd74ad38884d33bca65e52e",
      "5beb6e02aa3e4d58a7ecee4711e339bd",
      "e85f661f92d74c37889fcfa7a54259df",
      "93681925d02e49a0b4dc759de5d7ef24",
      "ad9a701dc4654bc8900eb68cee896cf1",
      "af562918e55149d9b0c125db6e559be6",
      "0b1cad1a52e3449e9019921e1f73465f",
      "f504b9ee5b304e30bcc349884809de61",
      "f3536dd2c71a41a3bcd0a4d0e3301a47",
      "8f621bb8bcb944359329f695327afb51",
      "44d2c5e6dc944fdcb7dfdbb405cfad6e",
      "42ddd1b4d4904590bafa469fddf0e2f5",
      "4d35f4b94e1b4221a8552d2ec8e17d31",
      "5a85e96330d54197819d9439110dd892",
      "f19b078360d74347b33f61ae599857a0"
     ]
    },
    "id": "vLQVR9tC9X_y",
    "outputId": "45c6e642-f12d-4b07-995b-70d3906024bb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14118b4463054fdab70515a3b83950d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a48039ecb149d18248baebb65013e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b9a71b42ae404082ed258fbad6bbf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba569b9a1910445db923271d96112b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd46ca356a64dcbaf727e0107fb939c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9a701dc4654bc8900eb68cee896cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"context\"], examples[\"question\"], truncation=True, padding=\"max_length\", max_length=2048)\n",
    "\n",
    "# Apply the tokenization function to the dataset\n",
    "dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ahIAXkkD9Zre"
   },
   "outputs": [],
   "source": [
    "# Ensure formatting\n",
    "def formatting_func(example):\n",
    "    context = example.get(\"context\", \"\")\n",
    "    question = example.get(\"question\", \"\")\n",
    "\n",
    "    try:\n",
    "        if isinstance(example[\"answers\"], dict):\n",
    "            answer_list = example[\"answers\"].get(\"text\", [])\n",
    "        elif isinstance(example[\"answers\"], list):\n",
    "            answer_list = example[\"answers\"][0].get(\"text\", []) if example[\"answers\"] else []\n",
    "        else:\n",
    "            answer_list = []\n",
    "\n",
    "        answer = answer_list[0] if answer_list else \"No answer\"\n",
    "    except Exception as e:\n",
    "        answer = \"No answer\"\n",
    "\n",
    "    # Return a list containing the formatted string\n",
    "    return [f\"\"\"Context: {context}\n",
    "Question: {question}\n",
    "Answer: {answer}\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LrMZjG0C9ZpA",
    "outputId": "34ed67de-3eee-41e1-a1be-3a24e8d0cdb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "# Add LoRA to model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "V0uoVhy09ZmX"
   },
   "outputs": [],
   "source": [
    "# Now apply to the model training as before\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    formatting_func=formatting_func,\n",
    "    dataset_text_field=None,\n",
    "    max_seq_length=2048,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        save_steps=200,\n",
    "        output_dir=\"qa_outputs\",\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "-4iHHZ2z9eQn",
    "outputId": "a95d2764-2ed7-43f5-a0c4-0ae94cfd3bf2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 87,599 | Num Epochs = 1 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 19,611,648/2,000,000,000 (0.98% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 25:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>9.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>9.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>9.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>8.875300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>8.852100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>8.670700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>8.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>8.331800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=8.89492332458496, metrics={'train_runtime': 1550.6976, 'train_samples_per_second': 0.516, 'train_steps_per_second': 0.064, 'total_flos': 1.96755069075456e+16, 'train_loss': 8.89492332458496})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5S2ilsyg9foz",
    "outputId": "d7ed990a-9f1b-494a-fbed-d8639046089a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine-tuned-gemma-qa/tokenizer_config.json',\n",
       " 'fine-tuned-gemma-qa/special_tokens_map.json',\n",
       " 'fine-tuned-gemma-qa/vocab.txt',\n",
       " 'fine-tuned-gemma-qa/added_tokens.json',\n",
       " 'fine-tuned-gemma-qa/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final model\n",
    "model.save_pretrained(\"fine-tuned-gemma-qa\")\n",
    "tokenizer.save_pretrained(\"fine-tuned-gemma-qa\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
